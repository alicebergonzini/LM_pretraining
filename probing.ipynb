{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers.pipelines import pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Si caricano i training set e i validation set per effettuare il probing. I dataset contengono la frase con le rispettive feature linguistiche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caricamento dataset\n",
    "train_df = pd.read_csv(\"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/probing_data/probing_train.csv\")\n",
    "val_df = pd.read_csv(\"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/probing_data/probing_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trasformazione in dizionario\n",
    "train_dict = train_df.to_dict(orient='records')\n",
    "val_dict = val_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBING\n",
    "\n",
    "Si definiscono delle funzioni per l'estrazione degli hidden states dei modelli per poi passarli al modello Ridge Regression come features per il task di regressione sulle caratteristiche linguistiche delle frasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione per ottenere gli embedding delle frasi.\n",
    "def feature_extraction(samples, model_name):\n",
    "    first_layer = 1\n",
    "    last_layer = 8\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-italian-cased\")\n",
    "    model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    for sample in tqdm(samples, desc=\"Estrazione features\", unit=\"sample\"):\n",
    "        encoded_sen = tokenizer(sample[\"sent\"], padding=True, truncation=True, max_length=128, return_tensors='pt') \n",
    "        with torch.no_grad():    \n",
    "            model_output = model(**encoded_sen, output_hidden_states=True)\n",
    "            hidden_states = model_output.hidden_states\n",
    "            for layer in range(first_layer, last_layer+1):\n",
    "                layer_output = torch.squeeze(hidden_states[layer])\n",
    "                cls_embedding = layer_output[0, :].cpu().detach().numpy()\n",
    "                sample[f'layer_{layer}'] = {'cls_embedding': cls_embedding}\n",
    "    return samples\n",
    "\n",
    "#funzione per ottenere features e lables\n",
    "def get_features_lables(samples, feature, layer):\n",
    "    X = []\n",
    "    y = []\n",
    "    for sample in samples:\n",
    "        emb = sample[layer][\"cls_embedding\"]\n",
    "        label =  sample[feature]\n",
    "        X.append(emb)\n",
    "        y.append(label)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "#funzione per addestrare e valutare il modello\n",
    "def train_eval(train_set, val_set, feature, layer):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train, y_train = get_features_lables(train_set, feature, layer)\n",
    "    X_val, y_val = get_features_lables(val_set, feature, layer)\n",
    "    X_train = np.array(X_train) \n",
    "    X_val = np.array(X_val)\n",
    "    scaled_X_train = scaler.fit_transform(X_train)\n",
    "    scaled_X_val = scaler.transform(X_val)\n",
    "    clf = sklearn.linear_model.Ridge(alpha=1.0)\n",
    "    clf.fit(scaled_X_train, y_train)\n",
    "    y_pred = clf.predict(scaled_X_val) \n",
    "    #return compute_metrics(np.array(y_val), y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoints = [2, 32, 512, 8192, 0]  #step su cui iterare\n",
    "ling_features = [\"n_tokens\",  \"char_per_tok\", \"upos_dist_DET\", \"upos_dist_ADV\", \"upos_dist_PUNCT\", \"upos_dist_NUM\", \"upos_dist_PRON\", \"upos_dist_ADP\", \"upos_dist_PROPN\",\"upos_dist_ADJ\",\"upos_dist_VERB\",\"upos_dist_NOUN\", \"upos_dist_CCONJ\", \"upos_dist_AUX\", \"avg_links_len\", \"max_links_len\", \"avg_max_depth\", \"dep_dist_obj\", \"dep_dist_nsubj\", \"subj_pre\", \"subj_post\", \"n_prepositional_chains\", \"avg_prepositional_chain_len\", \"avg_subordinate_chain_len\", \"subordinate_proposition_dist\", \"avg_verb_edges\"] #features di LP selezionate\n",
    "training_id = \"CURRICULUM\" #id del training usato sui modelli "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probing_checkpoints(checkpoints, training_id, train_dict, val_dict, ling_features):\n",
    "    first_layer = 1\n",
    "    last_layer = 8\n",
    "    results = pd.DataFrame()\n",
    "    for n_step in checkpoints:    #iteriamo sugli step selezionati\n",
    "        checkpoint_name = f'checkpoint-step{n_step}'\n",
    "        #selezioniamo il modello corretto allo step e con il training id giusti\n",
    "        if n_step == 0:  #lo 0 identifica l'ultimo step in questo caso\n",
    "            model_name = f\"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/models/{training_id}/final_pretrained_model\"\n",
    "            checkpoint = 15449\n",
    "            print(\"Inizio probing per il modello finale\")\n",
    "        else:\n",
    "            checkpoint = n_step\n",
    "            model_name = f\"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/models/{training_id}/checkpoints/{checkpoint_name}\"\n",
    "            print(f\"Inizio probing per il checkpoint {n_step}\")\n",
    "        print(\"Estrazione delle feature di training...\")\n",
    "        train_samples = feature_extraction(train_dict, model_name)  #si effettua l'estrazione delle feature per il checkpoint\n",
    "        print(\"Estrazione delle features di validation...\")\n",
    "        val_samples = feature_extraction(val_dict, model_name)\n",
    "        for ling_feature in ling_features:    #addestramento e validation su ogni feature linguistica del dataset\n",
    "            print(f'Addestramento del modello sulla feature linguistica: {ling_feature}') \n",
    "            for layer in range(first_layer, last_layer+1):\n",
    "                layer_result = train_eval(train_samples, val_samples, ling_feature, f'layer_{layer}')   #addestriamo il ridge per ogni layer ottenendo le metriche\n",
    "                row = {\"model\": training_id, \"step\": checkpoint, \"ling_feature\": ling_feature, \"layer\": layer, \"preds\": layer_result}\n",
    "                results = results._append(row, ignore_index = True)         \n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = probing_checkpoints(checkpoints, training_id, train_dict, val_dict, ling_features) #esecuzione probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#si salvano i risultati in un json\n",
    "result = final_results.to_json(f'C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/models/{training_id}/predictions.json', orient=\"columns\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
