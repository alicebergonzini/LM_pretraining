{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers.pipelines import pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/probing_data/probing_train.csv\")\n",
    "val_df = pd.read_csv(\"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/probing_data/probing_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = train_df.to_dict(orient='records')\n",
    "val_dict = val_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'isdt_tut-689',\n",
       " 'sent': \"Il diritto dell'usufruttuario non si estende al tesoro che si scopra durante l'usufrutto, salve le ragioni che gli possono competere come ritrovatore (932).\",\n",
       " 'category': 8,\n",
       " 'n_tokens': 31,\n",
       " 'char_per_tok': 4.81481481481482,\n",
       " 'upos_dist_DET': 16.1290322580645,\n",
       " 'upos_dist_ADV': 3.2258064516129,\n",
       " 'upos_dist_PUNCT': 12.9032258064516,\n",
       " 'upos_dist_NUM': 3.2258064516129,\n",
       " 'upos_dist_PRON': 16.1290322580645,\n",
       " 'upos_dist_ADP': 12.9032258064516,\n",
       " 'upos_dist_PROPN': 0.0,\n",
       " 'upos_dist_ADJ': 3.2258064516129,\n",
       " 'upos_dist_VERB': 9.67741935483871,\n",
       " 'upos_dist_NOUN': 19.3548387096774,\n",
       " 'upos_dist_CCONJ': 0.0,\n",
       " 'upos_dist_AUX': 3.2258064516129,\n",
       " 'avg_links_len': 2.42307692307692,\n",
       " 'max_links_len': 11,\n",
       " 'avg_max_depth': 5,\n",
       " 'dep_dist_obj': 0.0,\n",
       " 'dep_dist_nsubj': 12.9032258064516,\n",
       " 'subj_pre': 75.0,\n",
       " 'subj_post': 25.0,\n",
       " 'n_prepositional_chains': 1,\n",
       " 'avg_prepositional_chain_len': 1.0,\n",
       " 'avg_subordinate_chain_len': 1.0,\n",
       " 'subordinate_proposition_dist': 66.6666666666667,\n",
       " 'avg_verb_edges': 3.66666666666667}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione per ottenere gli embedding delle frasi.\n",
    "def feature_extraction(samples, model_name):\n",
    "    first_layer = 1\n",
    "    last_layer = 8\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-italian-cased\") #come tokenizzatore utilizziamo il solito giusto (bert-base-italian-uncased)? \n",
    "    model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    for sample in tqdm(samples, desc=\"Estrazione features\", unit=\"sample\"):\n",
    "        encoded_sen = tokenizer(sample[\"sent\"], padding=True, truncation=True, max_length=128, return_tensors='pt') \n",
    "        with torch.no_grad():    \n",
    "            model_output = model(**encoded_sen, output_hidden_states=True)\n",
    "            hidden_states = model_output.hidden_states\n",
    "            for layer in range(first_layer, last_layer+1):\n",
    "                layer_output = torch.squeeze(hidden_states[layer])\n",
    "                cls_embedding = layer_output[0, :].cpu().detach().numpy()\n",
    "                sample[f'layer_{layer}'] = {'cls_embedding': cls_embedding}\n",
    "    return samples\n",
    "\n",
    "#funzione per ottenere features e lables\n",
    "def get_features_lables(samples, feature, layer):\n",
    "    X = []\n",
    "    y = []\n",
    "    for sample in samples:\n",
    "        emb = sample[layer][\"cls_embedding\"]\n",
    "        label =  sample[feature]\n",
    "        X.append(emb)\n",
    "        y.append(label)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "#funzione per addestrare e valutare il modello\n",
    "def train_eval(train_set, val_set, feature, layer):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train, y_train = get_features_lables(train_set, feature, layer)\n",
    "    X_val, y_val = get_features_lables(val_set, feature, layer)\n",
    "    X_train = np.array(X_train) \n",
    "    X_val = np.array(X_val)\n",
    "    scaled_X_train = scaler.fit_transform(X_train)\n",
    "    scaled_X_val = scaler.transform(X_val)\n",
    "    clf = sklearn.linear_model.Ridge(alpha=1.0)\n",
    "    clf.fit(scaled_X_train, y_train)\n",
    "    y_pred = clf.predict(scaled_X_val) \n",
    "    #return compute_metrics(np.array(y_val), y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVO ITERARE ANCHE SUI CHECKPOINT CHE USIAMO\n",
    "checkpoints = [2, 32, 512, 8192, 0] \n",
    "ling_features = [\"n_tokens\",  \"char_per_tok\", \"upos_dist_DET\", \"upos_dist_ADV\", \"upos_dist_PUNCT\", \"upos_dist_NUM\", \"upos_dist_PRON\", \"upos_dist_ADP\", \"upos_dist_PROPN\",\"upos_dist_ADJ\",\"upos_dist_VERB\",\"upos_dist_NOUN\", \"upos_dist_CCONJ\", \"upos_dist_AUX\", \"avg_links_len\", \"max_links_len\", \"avg_max_depth\", \"dep_dist_obj\", \"dep_dist_nsubj\", \"subj_pre\", \"subj_post\", \"n_prepositional_chains\", \"avg_prepositional_chain_len\", \"avg_subordinate_chain_len\", \"subordinate_proposition_dist\", \"avg_verb_edges\"]\n",
    "training_id = \"RANDOM_S21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probing_checkpoints(checkpoints, training_id, train_dict, val_dict, ling_features):\n",
    "    first_layer = 1\n",
    "    last_layer = 8\n",
    "    results = pd.DataFrame()\n",
    "    for n_step in checkpoints:\n",
    "        checkpoint_name = f'checkpoint-step{n_step}'\n",
    "        if n_step == 0:\n",
    "            model_name = f\"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/models/{training_id}/final_pretrained_model\"\n",
    "            checkpoint = 15449\n",
    "            print(\"Inizio probing per il modello finale\")\n",
    "        else:\n",
    "            checkpoint = n_step\n",
    "            model_name = f\"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/models/{training_id}/checkpoints/{checkpoint_name}\"\n",
    "            print(f\"Inizio probing per il checkpoint {n_step}\")\n",
    "        print(\"Estrazione delle feature di training...\")\n",
    "        train_samples = feature_extraction(train_dict, model_name)  #si effettua l'estrazione delle feature per il checkpoint\n",
    "        print(\"Estrazione delle features di validation...\")\n",
    "        val_samples = feature_extraction(val_dict, model_name)\n",
    "        for ling_feature in ling_features:\n",
    "            print(f'Addestramento del modello sulla feature linguistica: {ling_feature}') \n",
    "            # layer_results = dict()\n",
    "            # feature_result = dict()\n",
    "            for layer in range(first_layer, last_layer+1):\n",
    "                #print(f\"Training for layer {layer}/{last_layer}\")\n",
    "                layer_result = train_eval(train_samples, val_samples, ling_feature, f'layer_{layer}')   #addestriamo il ridge per ogni layer ottenendo le metriche\n",
    "                row = {\"model\": training_id, \"step\": checkpoint, \"ling_feature\": ling_feature, \"layer\": layer, \"preds\": layer_result}\n",
    "                results = results._append(row, ignore_index = True)            #     layer_results[f'layer_{layer}'] = layer_result\n",
    "            # feature_result[ling_feature] = {\"results\": layer_results}\n",
    "            # results[f\"checkpoint{n_step}\"] = feature_result\n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizio probing per il checkpoint 2\n",
      "Estrazione delle feature di training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estrazione features: 100%|██████████| 10000/10000 [10:04<00:00, 16.55sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estrazione delle features di validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estrazione features: 100%|██████████| 5000/5000 [05:03<00:00, 16.49sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addestramento del modello sulla feature linguistica: n_tokens\n",
      "Addestramento del modello sulla feature linguistica: char_per_tok\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_DET\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_ADV\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_PUNCT\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_NUM\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_PRON\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_ADP\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_PROPN\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_ADJ\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_VERB\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_NOUN\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_CCONJ\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_AUX\n",
      "Addestramento del modello sulla feature linguistica: avg_links_len\n",
      "Addestramento del modello sulla feature linguistica: max_links_len\n",
      "Addestramento del modello sulla feature linguistica: avg_max_depth\n",
      "Addestramento del modello sulla feature linguistica: dep_dist_obj\n",
      "Addestramento del modello sulla feature linguistica: dep_dist_nsubj\n",
      "Addestramento del modello sulla feature linguistica: subj_pre\n",
      "Addestramento del modello sulla feature linguistica: subj_post\n",
      "Addestramento del modello sulla feature linguistica: n_prepositional_chains\n",
      "Addestramento del modello sulla feature linguistica: avg_prepositional_chain_len\n",
      "Addestramento del modello sulla feature linguistica: avg_subordinate_chain_len\n",
      "Addestramento del modello sulla feature linguistica: subordinate_proposition_dist\n",
      "Addestramento del modello sulla feature linguistica: avg_verb_edges\n",
      "Inizio probing per il checkpoint 32\n",
      "Estrazione delle feature di training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estrazione features: 100%|██████████| 10000/10000 [11:49<00:00, 14.09sample/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estrazione delle features di validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estrazione features: 100%|██████████| 5000/5000 [05:33<00:00, 15.00sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addestramento del modello sulla feature linguistica: n_tokens\n",
      "Addestramento del modello sulla feature linguistica: char_per_tok\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_DET\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_ADV\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_PUNCT\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_NUM\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_PRON\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_ADP\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_PROPN\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_ADJ\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_VERB\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_NOUN\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_CCONJ\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_AUX\n",
      "Addestramento del modello sulla feature linguistica: avg_links_len\n",
      "Addestramento del modello sulla feature linguistica: max_links_len\n",
      "Addestramento del modello sulla feature linguistica: avg_max_depth\n",
      "Addestramento del modello sulla feature linguistica: dep_dist_obj\n",
      "Addestramento del modello sulla feature linguistica: dep_dist_nsubj\n",
      "Addestramento del modello sulla feature linguistica: subj_pre\n",
      "Addestramento del modello sulla feature linguistica: subj_post\n",
      "Addestramento del modello sulla feature linguistica: n_prepositional_chains\n",
      "Addestramento del modello sulla feature linguistica: avg_prepositional_chain_len\n",
      "Addestramento del modello sulla feature linguistica: avg_subordinate_chain_len\n",
      "Addestramento del modello sulla feature linguistica: subordinate_proposition_dist\n",
      "Addestramento del modello sulla feature linguistica: avg_verb_edges\n",
      "Inizio probing per il checkpoint 512\n",
      "Estrazione delle feature di training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estrazione features: 100%|██████████| 10000/10000 [10:34<00:00, 15.76sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estrazione delle features di validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estrazione features: 100%|██████████| 5000/5000 [05:07<00:00, 16.28sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addestramento del modello sulla feature linguistica: n_tokens\n",
      "Addestramento del modello sulla feature linguistica: char_per_tok\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_DET\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_ADV\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_PUNCT\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_NUM\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_PRON\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_ADP\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_PROPN\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_ADJ\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_VERB\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_NOUN\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_CCONJ\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_AUX\n",
      "Addestramento del modello sulla feature linguistica: avg_links_len\n",
      "Addestramento del modello sulla feature linguistica: max_links_len\n",
      "Addestramento del modello sulla feature linguistica: avg_max_depth\n",
      "Addestramento del modello sulla feature linguistica: dep_dist_obj\n",
      "Addestramento del modello sulla feature linguistica: dep_dist_nsubj\n",
      "Addestramento del modello sulla feature linguistica: subj_pre\n",
      "Addestramento del modello sulla feature linguistica: subj_post\n",
      "Addestramento del modello sulla feature linguistica: n_prepositional_chains\n",
      "Addestramento del modello sulla feature linguistica: avg_prepositional_chain_len\n",
      "Addestramento del modello sulla feature linguistica: avg_subordinate_chain_len\n",
      "Addestramento del modello sulla feature linguistica: subordinate_proposition_dist\n",
      "Addestramento del modello sulla feature linguistica: avg_verb_edges\n",
      "Inizio probing per il checkpoint 8192\n",
      "Estrazione delle feature di training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estrazione features: 100%|██████████| 10000/10000 [10:45<00:00, 15.49sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estrazione delle features di validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estrazione features: 100%|██████████| 5000/5000 [05:33<00:00, 14.97sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addestramento del modello sulla feature linguistica: n_tokens\n",
      "Addestramento del modello sulla feature linguistica: char_per_tok\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_DET\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_ADV\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_PUNCT\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_NUM\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_PRON\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_ADP\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_PROPN\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_ADJ\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_VERB\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_NOUN\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_CCONJ\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_AUX\n",
      "Addestramento del modello sulla feature linguistica: avg_links_len\n",
      "Addestramento del modello sulla feature linguistica: max_links_len\n",
      "Addestramento del modello sulla feature linguistica: avg_max_depth\n",
      "Addestramento del modello sulla feature linguistica: dep_dist_obj\n",
      "Addestramento del modello sulla feature linguistica: dep_dist_nsubj\n",
      "Addestramento del modello sulla feature linguistica: subj_pre\n",
      "Addestramento del modello sulla feature linguistica: subj_post\n",
      "Addestramento del modello sulla feature linguistica: n_prepositional_chains\n",
      "Addestramento del modello sulla feature linguistica: avg_prepositional_chain_len\n",
      "Addestramento del modello sulla feature linguistica: avg_subordinate_chain_len\n",
      "Addestramento del modello sulla feature linguistica: subordinate_proposition_dist\n",
      "Addestramento del modello sulla feature linguistica: avg_verb_edges\n",
      "Inizio probing per il modello finale\n",
      "Estrazione delle feature di training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estrazione features: 100%|██████████| 10000/10000 [11:00<00:00, 15.14sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estrazione delle features di validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estrazione features: 100%|██████████| 5000/5000 [05:11<00:00, 16.05sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addestramento del modello sulla feature linguistica: n_tokens\n",
      "Addestramento del modello sulla feature linguistica: char_per_tok\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_DET\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_ADV\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_PUNCT\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_NUM\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_PRON\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_ADP\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_PROPN\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_ADJ\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_VERB\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_NOUN\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_CCONJ\n",
      "Addestramento del modello sulla feature linguistica: upos_dist_AUX\n",
      "Addestramento del modello sulla feature linguistica: avg_links_len\n",
      "Addestramento del modello sulla feature linguistica: max_links_len\n",
      "Addestramento del modello sulla feature linguistica: avg_max_depth\n",
      "Addestramento del modello sulla feature linguistica: dep_dist_obj\n",
      "Addestramento del modello sulla feature linguistica: dep_dist_nsubj\n",
      "Addestramento del modello sulla feature linguistica: subj_pre\n",
      "Addestramento del modello sulla feature linguistica: subj_post\n",
      "Addestramento del modello sulla feature linguistica: n_prepositional_chains\n",
      "Addestramento del modello sulla feature linguistica: avg_prepositional_chain_len\n",
      "Addestramento del modello sulla feature linguistica: avg_subordinate_chain_len\n",
      "Addestramento del modello sulla feature linguistica: subordinate_proposition_dist\n",
      "Addestramento del modello sulla feature linguistica: avg_verb_edges\n"
     ]
    }
   ],
   "source": [
    "final_results = probing_checkpoints(checkpoints, training_id, train_dict, val_dict, ling_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>step</th>\n",
       "      <th>ling_feature</th>\n",
       "      <th>layer</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RANDOM_S21</td>\n",
       "      <td>2</td>\n",
       "      <td>n_tokens</td>\n",
       "      <td>1</td>\n",
       "      <td>[12.669121, 28.722866, 17.334045, 26.084396, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RANDOM_S21</td>\n",
       "      <td>2</td>\n",
       "      <td>n_tokens</td>\n",
       "      <td>2</td>\n",
       "      <td>[12.749468, 27.010532, 16.193115, 27.377407, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RANDOM_S21</td>\n",
       "      <td>2</td>\n",
       "      <td>n_tokens</td>\n",
       "      <td>3</td>\n",
       "      <td>[11.068756, 28.734419, 17.039028, 25.997948, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RANDOM_S21</td>\n",
       "      <td>2</td>\n",
       "      <td>n_tokens</td>\n",
       "      <td>4</td>\n",
       "      <td>[12.695541, 29.03796, 18.320244, 26.494717, 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RANDOM_S21</td>\n",
       "      <td>2</td>\n",
       "      <td>n_tokens</td>\n",
       "      <td>5</td>\n",
       "      <td>[14.834208, 28.661907, 17.968287, 24.597189, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model  step ling_feature  layer  \\\n",
       "0  RANDOM_S21     2     n_tokens      1   \n",
       "1  RANDOM_S21     2     n_tokens      2   \n",
       "2  RANDOM_S21     2     n_tokens      3   \n",
       "3  RANDOM_S21     2     n_tokens      4   \n",
       "4  RANDOM_S21     2     n_tokens      5   \n",
       "\n",
       "                                               preds  \n",
       "0  [12.669121, 28.722866, 17.334045, 26.084396, 2...  \n",
       "1  [12.749468, 27.010532, 16.193115, 27.377407, 2...  \n",
       "2  [11.068756, 28.734419, 17.039028, 25.997948, 2...  \n",
       "3  [12.695541, 29.03796, 18.320244, 26.494717, 21...  \n",
       "4  [14.834208, 28.661907, 17.968287, 24.597189, 2...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "result = final_results.to_json(f'C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/models/{training_id}/predictions.json', orient=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "not writable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/predictions.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f: \n\u001b[1;32m----> 4\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bergo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:180\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m--> 180\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: not writable"
     ]
    }
   ],
   "source": [
    "'''import json \n",
    "\n",
    "with open(f\"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/models/{training_id}/predictions.txt\") as f: \n",
    "    json.dump(result, f)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
