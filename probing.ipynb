{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers.pipelines import pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/probing_data/probing_train.csv\")\n",
    "val_df = pd.read_csv(\"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/probing_data/probing_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = train_df.to_dict(orient='records')\n",
    "val_dict = val_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'isdt_tut-689',\n",
       " 'sent': \"Il diritto dell'usufruttuario non si estende al tesoro che si scopra durante l'usufrutto, salve le ragioni che gli possono competere come ritrovatore (932).\",\n",
       " 'category': 8,\n",
       " 'n_tokens': 31,\n",
       " 'char_per_tok': 4.81481481481482,\n",
       " 'upos_dist_DET': 16.1290322580645,\n",
       " 'upos_dist_ADV': 3.2258064516129,\n",
       " 'upos_dist_PUNCT': 12.9032258064516,\n",
       " 'upos_dist_NUM': 3.2258064516129,\n",
       " 'upos_dist_PRON': 16.1290322580645,\n",
       " 'upos_dist_ADP': 12.9032258064516,\n",
       " 'upos_dist_PROPN': 0.0,\n",
       " 'upos_dist_ADJ': 3.2258064516129,\n",
       " 'upos_dist_VERB': 9.67741935483871,\n",
       " 'upos_dist_NOUN': 19.3548387096774,\n",
       " 'upos_dist_CCONJ': 0.0,\n",
       " 'upos_dist_AUX': 3.2258064516129,\n",
       " 'avg_links_len': 2.42307692307692,\n",
       " 'max_links_len': 11,\n",
       " 'avg_max_depth': 5,\n",
       " 'dep_dist_obj': 0.0,\n",
       " 'dep_dist_nsubj': 12.9032258064516,\n",
       " 'subj_pre': 75.0,\n",
       " 'subj_post': 25.0,\n",
       " 'n_prepositional_chains': 1,\n",
       " 'avg_prepositional_chain_len': 1.0,\n",
       " 'avg_subordinate_chain_len': 1.0,\n",
       " 'subordinate_proposition_dist': 66.6666666666667,\n",
       " 'avg_verb_edges': 3.66666666666667}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione per ottenere gli embedding delle frasi.\n",
    "def feature_extraction(samples, model_name):\n",
    "    first_layer = 1\n",
    "    last_layer = 8\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-italian-cased\") #come tokenizzatore utilizziamo il solito giusto (bert-base-italian-uncased)? \n",
    "    model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    for sample in tqdm(samples, desc=\"Estrazione features\", unit=\"sample\"):\n",
    "        encoded_sen = tokenizer(sample[\"sent\"], padding=True, truncation=True, max_length=128, return_special_tokens_mask=True, return_tensors='pt') \n",
    "        with torch.no_grad():    \n",
    "            model_output = model(**encoded_sen, output_hidden_states=True)\n",
    "            hidden_states = model_output.hidden_states\n",
    "            for layer in range(first_layer, last_layer+1):\n",
    "                layer_output = torch.squeeze(hidden_states[layer])\n",
    "                cls_embedding = layer_output[0, :].cpu().detach().numpy()\n",
    "                sample[f'layer_{layer}'] = {'cls_embedding': cls_embedding}\n",
    "    return samples\n",
    "\n",
    "#funzione per ottenere features e lables\n",
    "def get_features_lables(samples, feature, layer):\n",
    "    X = []\n",
    "    y = []\n",
    "    for sample in samples:\n",
    "        X = sample[layer][\"cls_embedding\"]\n",
    "        y = sample[feature]\n",
    "    return X, y\n",
    "\n",
    "#funzione per addestrare e valutare il modello\n",
    "def train_eval(train_set, val_set, feature, layer):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train, y_train = get_features_lables(train_set, feature, layer)\n",
    "    X_val, y_val = get_features_lables(val_set)\n",
    "    scaled_X_train = scaler.fit_transform(X_train)\n",
    "    scaled_X_val = scaler.transform(X_val)\n",
    "    clf = sklearn.linear_model.Ridge(alpha=1.0)\n",
    "    clf.fit(scaled_X_train, y_train)\n",
    "    y_pred = clf.predict(scaled_X_val)\n",
    "    return classification_report(y_val, y_pred, output_dict=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estrazione features: 100%|██████████| 10000/10000 [08:33<00:00, 19.46sample/s]\n"
     ]
    }
   ],
   "source": [
    "'''final_model_path =  f\"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/models/ANTI_CURRICULUM/checkpoints/checkpoint-step15625\"\n",
    "samples = feature_extraction(data_dict, final_model_path)'''\"DOMANDE .txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il diritto dell'usufruttuario non si estende al tesoro che si scopra durante l'usufrutto, salve le ragioni che gli possono competere come ritrovatore (932).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVO ITERARE ANCHE SUI CHECKPOINT CHE USIAMO\n",
    "checkpoints = [2, 32, 512, 8192, 15625] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probing_checkpoints(checkpoints, training_id, train_dict, val_dict):\n",
    "    first_layer = 1\n",
    "    last_layer = 8\n",
    "    for n_step in checkpoints:\n",
    "        checkpoint_name = f'checkpoint-step{n_step}'\n",
    "        model_name = f\"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/models/{training_id}/checkpoints/{checkpoint_name}\"\n",
    "        train_samples = feature_extraction(train_dict, model_name)\n",
    "        val_samples = feature_extraction(val_dict, model_name)\n",
    "        #train_eval()\n",
    "        for layer in range(first_layer, last_layer+1):\n",
    "            results = train_eval(train_samples, val_samples, \"n_tokens\", f'layer_{layer}')\n",
    "            print(results)\n",
    "            break\n",
    "        break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/models/ANTI_CURRICULUM/checkpoints/checkpoint-step2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprobing_checkpoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mANTI_CURRICULUM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [81]\u001b[0m, in \u001b[0;36mprobing_checkpoints\u001b[1;34m(checkpoints, training_id, train_dict, val_dict)\u001b[0m\n\u001b[0;32m      5\u001b[0m checkpoint_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint-step\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/checkpoints/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m train_samples \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m val_samples \u001b[38;5;241m=\u001b[39m feature_extraction(val_dict, model_name)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#train_eval()\u001b[39;00m\n",
      "Input \u001b[1;32mIn [57]\u001b[0m, in \u001b[0;36mfeature_extraction\u001b[1;34m(samples, model_name)\u001b[0m\n\u001b[0;32m      4\u001b[0m last_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbmdz/bert-base-italian-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#come tokenizzatore utilizziamo il solito giusto (bert-base-italian-uncased)? \u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForMaskedLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m tqdm(samples, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstrazione features\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      8\u001b[0m     encoded_sen \u001b[38;5;241m=\u001b[39m tokenizer(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent\u001b[39m\u001b[38;5;124m\"\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "File \u001b[1;32mc:\\Users\\bergo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:3305\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   3301\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME, variant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3302\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3303\u001b[0m         )\n\u001b[0;32m   3304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   3306\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME, variant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME, variant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3307\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3308\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3309\u001b[0m         )\n\u001b[0;32m   3310\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subfolder, pretrained_model_name_or_path)):\n\u001b[0;32m   3311\u001b[0m     archive_file \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n",
      "\u001b[1;31mOSError\u001b[0m: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/models/ANTI_CURRICULUM/checkpoints/checkpoint-step2."
     ]
    }
   ],
   "source": [
    "probing_checkpoints(checkpoints, \"ANTI_CURRICULUM\", train_dict, val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
