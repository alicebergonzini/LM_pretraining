{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd41ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7023f7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setto il device da usare\n",
    "torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c3a13b",
   "metadata": {},
   "source": [
    "### PRE-PROCESSING\n",
    "\n",
    "Per ottenere un dataset che possa essere utilizzato come training del nostro language model partiamo dai file conllu che contengono i testi annotati di wikipedia italiana. Da questi file vogliamo ottenere una struttura dati che per ogni frase riporti id, testo e indice di Gulpease (per ora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "24b16f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data\\\\prova.conllu', 'data\\\\prova2.conllu', 'data\\\\text_all.txt']\n"
     ]
    }
   ],
   "source": [
    "#si ottengono i path di ogni file per il pretraining e si salvano in una lista\n",
    "ds_directory = \"data\"\n",
    "ds_files = []\n",
    "for file_name in os.listdir(ds_directory):\n",
    "    file_path = os.path.join(ds_directory, file_name)\n",
    "    ds_files.append(file_path)\n",
    "print(ds_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c8b022a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione che calcola gulpease - io in teoria userò read-it\n",
    "def comp_gulpease(ns, nw, nl):\n",
    "    g_value = 89 + ((300*ns - 10*nl)/nw) #è corretta questa formula?\n",
    "    return g_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c22fe300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione che legge i file conllu riga per riga, estraendo id, testo e indice di complessità di ciascuna frase \n",
    "def extract(file_path, n_file):\n",
    "    id_list = []\n",
    "    text_list = []\n",
    "    gulp_list = []\n",
    "    current_id=\"\"\n",
    "    current_sent=\"\"\n",
    "    for line in open(file_path, 'r', encoding='utf-8'):\n",
    "        if line.startswith(\"# text\"):\n",
    "            current_sent = line[9:].rstrip('\\n')\n",
    "            words = current_sent.split()\n",
    "            gulp = comp_gulpease(1, len(words), sum(len(word) for word in words))\n",
    "            text_list.append(current_sent)\n",
    "            gulp_list.append(gulp)\n",
    "        elif line.startswith(\"# sent_id\"):\n",
    "            current_id = re.sub(r'\\D', '', line)\n",
    "            id_list.append(f'{current_id}_{str(n_file)}') #per avere id univoco ho aggiunto numero del file\n",
    "    return id_list, text_list, gulp_list\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a84d81c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#si estraggono id e testo tramite le funzioni sopra definite\n",
    "n_file = 1\n",
    "id_list = []\n",
    "text_list = []\n",
    "gulp_list = []\n",
    "for item in ds_files:\n",
    "    item_ids, item_texts, item_gulp = extract(item, n_file)\n",
    "    id_list = id_list + item_ids\n",
    "    text_list = text_list + item_texts\n",
    "    gulp_list = gulp_list + item_gulp\n",
    "    n_file += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2d5cc5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>gulp_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_1</td>\n",
       "      <td>L'allunaggio è la discesa di un veicolo sulla ...</td>\n",
       "      <td>74.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2_1</td>\n",
       "      <td>Si distingue tra allunaggio duro, cioè un impa...</td>\n",
       "      <td>42.703704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3_1</td>\n",
       "      <td>Il programma Luna, partito nel 1959 con la son...</td>\n",
       "      <td>56.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4_1</td>\n",
       "      <td>Luna 9, il 3 febbraio 1966, eseguì il primo at...</td>\n",
       "      <td>63.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5_1</td>\n",
       "      <td>Il primo allunaggio di un essere umano, il 20 ...</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                               text  gulp_index\n",
       "0  1_1  L'allunaggio è la discesa di un veicolo sulla ...   74.555556\n",
       "1  2_1  Si distingue tra allunaggio duro, cioè un impa...   42.703704\n",
       "2  3_1  Il programma Luna, partito nel 1959 con la son...   56.142857\n",
       "3  4_1  Luna 9, il 3 febbraio 1966, eseguì il primo at...   63.615385\n",
       "4  5_1  Il primo allunaggio di un essere umano, il 20 ...   45.000000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#si crea un dataframe con una riga per frase, attributi: id, testo e indice di gulpease\n",
    "ds_df = pd.DataFrame(columns=[\"id\", \"text\", \"gulp_index\"])\n",
    "\n",
    "ds_df[\"id\"] = id_list\n",
    "ds_df[\"text\"] = text_list\n",
    "ds_df[\"gulp_index\"]  = gulp_list\n",
    "\n",
    "ds_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7563c918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Divido in training e test set\n",
    "dataset = Dataset.from_pandas(ds_df)\n",
    "split_set = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_ds = split_set[\"train\"]\n",
    "test_ds = split_set[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e9bd3",
   "metadata": {},
   "source": [
    "### TOKENIZATION\n",
    "\n",
    "In questa sezione si importa il tokenizzatore col quale si tokenizza ciascuna frase nel formato necessario per Bert, alla fine si otterrà un dataset nel formato corretto con tutte le features necessarie per il training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cc991e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0317bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#si importa il tokenizzatore già configurato (in questo caso: bert-base-italian-cased)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-italian-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c6a74b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5357eca1f9144c4a43a4d5227dee7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/194 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2b5ef39f234d659e1ddef746b2b9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#facciamo l'encoding di tutto il dataset tokenizzando frase per frase\n",
    "def encode(sample):\n",
    "    return tokenizer(sample[\"text\"], padding=True, truncation=True, max_length=512, return_special_tokens_mask=True)\n",
    "\n",
    "train_set = train_ds.map(encode, batched=True)\n",
    "test_set = test_ds.map(encode, batched=True)\n",
    "train_set.set_format('torch', columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"])\n",
    "test_set.set_format('torch', columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "643587d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'gulp_index', 'input_ids', 'token_type_ids', 'special_tokens_mask', 'attention_mask'],\n",
       "    num_rows: 194\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "95290375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'gulp_index', 'input_ids', 'token_type_ids', 'special_tokens_mask', 'attention_mask'],\n",
       "    num_rows: 22\n",
       "})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def829e",
   "metadata": {},
   "source": [
    "### TRAINING DI BERT\n",
    "\n",
    "Si procede al training di Bert. Il modello dovrà partire da uno stato iniziale con pesi random, per questo non si importa il modello già addestrato, ma si configura semplicemente l'architettura la sua architettura per poi addestrarlo da zero. Si definisce poi una strategia di training e i suoi argomenti per poi addestrare il modello sul trask di Language Modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "93b20d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "54c37d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"prajjwal1/bert-mini\"\n",
    "model_config = BertConfig.from_pretrained(model_name)\n",
    "\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c2348df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(31102, 256)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForMaskedLM(model_config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "abd13ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1024,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 4,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.20.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 31102\n",
       "}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "09353207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usiamo il datacollator per fare le batch per il training\n",
    "datacollator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.2, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bc554e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=PreTrainedTokenizer(name_or_path='dbmdz/bert-base-italian-cased', vocab_size=31102, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), mlm=True, mlm_probability=0.2, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datacollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90322bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c0a3b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#argomenti provvisori, da definire meglio\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"my_pretrained_model\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a14c39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=datacollator,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0fc08b00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, id, special_tokens_mask, gulp_index. If text, id, special_tokens_mask, gulp_index are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\bergo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 194\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2de69e43c3241b3afff080a1c38c3e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
