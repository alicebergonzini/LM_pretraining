{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd41ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c3a13b",
   "metadata": {},
   "source": [
    "### PRE-PROCESSING\n",
    "\n",
    "Per ottenere un dataset che possa essere utilizzato come training del nostro language model partiamo dai file conllu che contengono i testi annotati di wikipedia italiana. Da questi file vogliamo ottenere una struttura dati che per ogni frase riporti id, testo e indice di Gulpease (per ora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "24b16f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data\\\\prova.conllu', 'data\\\\prova2.conllu', 'data\\\\text_all.txt']\n"
     ]
    }
   ],
   "source": [
    "#si ottengono i path di ogni file per il pretraining e si salvano in una lista\n",
    "\n",
    "ds_directory = \"data\"\n",
    "ds_files = []\n",
    "for file_name in os.listdir(ds_directory):\n",
    "    file_path = os.path.join(ds_directory, file_name)\n",
    "    ds_files.append(file_path)\n",
    "    \n",
    "print(ds_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ef03f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_id(file_path, n_file):\n",
    "    id_list = []\n",
    "    current_id = \"\"\n",
    "    for line in open(file_path, 'r',  encoding='utf-8'):\n",
    "        if line.startswith(\"# sent_id\"):\n",
    "            current_id = re.sub(r'\\D', '', line)\n",
    "            id_list.append(current_id + \"_\" + str(n_file))\n",
    "    return id_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "413c253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#PROBLEMA: l'id non è univoco se abbiamo un insieme di file che sono stati \"parsati\" singolarmente. \n",
    "#Possibile soluzione: aggiungere un numero identificativo del file. ?! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "716f6c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#le preposizioni articolate vanno divise in costituenti o va bene la frase intera? \n",
    "# -> intanto prendo la frase intera in caso cambio\n",
    "def get_text(file_path):\n",
    "    text_list = []\n",
    "    current_sent = \"\"\n",
    "    for line in open(file_path, 'r',  encoding='utf-8'): \n",
    "        if line.startswith(\"# text\"):\n",
    "            current_sent = line[9:].rstrip('\\n')\n",
    "            text_list.append(current_sent)\n",
    "    return text_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c8b022a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_gulpease(ns, nw, nl):\n",
    "    g_value = 89 + ((300*ns - 10*nl)/nw) #è corretta questa formula?\n",
    "    return g_value\n",
    "\n",
    "def get_gulpease(file_path):\n",
    "    gulp_list = []\n",
    "    for line in open(file_path, 'r', encoding = \"utf-8\"):  #questa riga la ripeto tre volte meglio ottimizzare in un'unica f\n",
    "        if line.startswith(\"# text\"):\n",
    "            current_sent = line[9:].rstrip('\\n')\n",
    "            words = current_sent.split()\n",
    "            gulp = comp_gulpease(1, len(words), sum(len(word) for word in words))\n",
    "            gulp_list.append(gulp)\n",
    "    return gulp_list\n",
    "\n",
    "#DOMANDA: l'indice di gulpease serve a determinare la difficoltà di un intero testo. Come fare se abbiamo una singola frase?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a84d81c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#si estraggono id e testo tramite le funzioni sopra definite\n",
    "\n",
    "id_list = []\n",
    "gulpease_list = []\n",
    "text_list = []\n",
    "n_file = 1\n",
    "for item in ds_files:\n",
    "    id_list = id_list + get_id(item, n_file)\n",
    "    n_file += 1\n",
    "    text_list = text_list + get_text(item)\n",
    "    gulpease_list = gulpease_list + get_gulpease(item)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2d5cc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#si crea un dataframe con una riga per frase, attributi: id, testo e indice di gulpease\n",
    "\n",
    "ds_df = pd.DataFrame(columns=[\"id\", \"text\", \"gulp_index\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4e483037",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df[\"id\"] = id_list\n",
    "ds_df[\"text\"] = text_list\n",
    "ds_df[\"gulp_index\"]  = gulpease_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "40a31cc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>gulp_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_1</td>\n",
       "      <td>L'allunaggio è la discesa di un veicolo sulla ...</td>\n",
       "      <td>74.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2_1</td>\n",
       "      <td>Si distingue tra allunaggio duro, cioè un impa...</td>\n",
       "      <td>42.703704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3_1</td>\n",
       "      <td>Il programma Luna, partito nel 1959 con la son...</td>\n",
       "      <td>56.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4_1</td>\n",
       "      <td>Luna 9, il 3 febbraio 1966, eseguì il primo at...</td>\n",
       "      <td>63.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5_1</td>\n",
       "      <td>Il primo allunaggio di un essere umano, il 20 ...</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                               text  gulp_index\n",
       "0  1_1  L'allunaggio è la discesa di un veicolo sulla ...   74.555556\n",
       "1  2_1  Si distingue tra allunaggio duro, cioè un impa...   42.703704\n",
       "2  3_1  Il programma Luna, partito nel 1959 con la son...   56.142857\n",
       "3  4_1  Luna 9, il 3 febbraio 1966, eseguì il primo at...   63.615385\n",
       "4  5_1  Il primo allunaggio di un essere umano, il 20 ...   45.000000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4af9f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    " # TO  DO \n",
    "#ottimizzare il codice: il for line in open... farlo in una funzione unica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7563c918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 172 entries, 84 to 102\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          172 non-null    object \n",
      " 1   text        172 non-null    object \n",
      " 2   gulp_index  172 non-null    float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 5.4+ KB\n"
     ]
    }
   ],
   "source": [
    "#Divido in training e test set\n",
    "train_df, test_df = train_test_split(ds_df, test_size=0.2, random_state=42)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e9bd3",
   "metadata": {},
   "source": [
    "### TOKENIZATION\n",
    "\n",
    "In questa sezione si importa il tokenizzatore col quale si tokenizza ciascuna frase nel formato necessario per Bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cc991e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5b6faab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per il tokenizer ho bisogno del testo in formato txt\n",
    "text = '\\n'.join(ds_df['text'])\n",
    "\n",
    "with open('data/text_all.txt', 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8ff53fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntokenizer = tokenizers.BertWordPieceTokenizer()\\n\\nfiles = [\"data/text_all.txt\"]\\n         \\ntokenizer.train(files=files, vocab_size=20000)#argomenti vanno bene? BOH\\ntokenizer.enable_truncation(max_length=256)'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dobbiamo fare il pre-training del tokenizer???????????\n",
    "'''\n",
    "tokenizer = tokenizers.BertWordPieceTokenizer()\n",
    "\n",
    "files = [\"data/text_all.txt\"]\n",
    "         \n",
    "tokenizer.train(files=files, vocab_size=20000)#argomenti vanno bene? BOH\n",
    "tokenizer.enable_truncation(max_length=256)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0317bee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/dbmdz/bert-base-italian-cased/resolve/main/vocab.txt from cache at C:\\Users\\bergo/.cache\\huggingface\\transformers\\e386d7030c11abe3c82da83b0aa728f3c09ab3a6728e325fe78bb5a0c67d7c71.83ca512ab51c5bc2809e83002a054b84ab85a200b98d5c0eb036d7611ee4362e\n",
      "loading file https://huggingface.co/dbmdz/bert-base-italian-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/dbmdz/bert-base-italian-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/dbmdz/bert-base-italian-cased/resolve/main/tokenizer_config.json from cache at C:\\Users\\bergo/.cache\\huggingface\\transformers\\534fa05777338ca7e2b068a37beb83688543de270a20252296be3eadd10caca1.6391beef2ceed2cdba47401eb12680200856c97d2f2b56143e515d7c0f36a66a\n",
      "loading configuration file https://huggingface.co/dbmdz/bert-base-italian-cased/resolve/main/config.json from cache at C:\\Users\\bergo/.cache\\huggingface\\transformers\\4641bcb7c4ac61788587ad50d2f1598c64a1c28a71631929524c234bcf1e422e.6ec690b98e01c56d26601258d2be34c3e5a76b949465ed58983cff81e5f9fa88\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dbmdz/bert-base-italian-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-italian-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c6a74b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#facciamo l'encoding di tutto il dataset tokenizzando frase per frase\n",
    "def encode(text):\n",
    "    return tokenizer(text, padding=True, truncation=True, max_length=256, return_special_tokens_mask=True)\n",
    "\n",
    "train_set = train_df[\"text\"].map(encode)\n",
    "test_set = test_df[\"text\"].map(encode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "24ba6ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [102, 178, 1553, 12932, 12332, 160, 198, 146, 14577, 120, 141, 6964, 602, 11456, 697, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f121ec9c",
   "metadata": {},
   "source": [
    "### TRAINING DI BERT\n",
    "\n",
    "Si procede al training di Bert. Il modello dovrà partire da uno stato iniziale con pesi random, per questo non si importa il modello già addestrato, ma si configura semplicemente l'architettura la sua architettura per poi addestrarlo da zero. Si definisce poi una strategia di training e i suoi argomenti per poi addestrare il modello sul trask di Language Modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c3e13fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e0f4e8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilizzeremo un bert piccolino per ora: prajjwal1/bert-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8f098ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/prajjwal1/bert-mini/resolve/main/config.json from cache at C:\\Users\\bergo/.cache\\huggingface\\transformers\\a32529b12a03c02e99c269bf68c0c7b8349093f626e860ab9b012e3d9539c539.e6c2a1d71adb3143ecd42222c4604e92ff255a7663c04bb5c4fad770c78e096c\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"prajjwal1/bert-mini\"\n",
    "model_config = BertConfig.from_pretrained(model_name)\n",
    "\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "949361fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c40ae8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usiamo il datacollator per fare le batch per il training\n",
    "datacollator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.2, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f8ae53df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#se si hanno più GPU installare accelarate, per ora uso la mia CPU \n",
    "#!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4e5c536e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 10\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "#argomenti provvisori, da definire meglio\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"my_pretrained_model\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=30,\n",
    "    save_total_limit=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e2428d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=10,\n",
       "evaluation_strategy=IntervalStrategy.STEPS,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_min_num_params=0,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=-1,\n",
       "log_level=-1,\n",
       "log_level_replica=-1,\n",
       "log_on_each_node=True,\n",
       "logging_dir=my_pretrained_model\\runs\\Apr15_13-17-07_LAPTOP-K68KFNTC,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=10,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=2,\n",
       "optim=OptimizerNames.ADAMW_HF,\n",
       "output_dir=my_pretrained_model,\n",
       "overwrite_output_dir=True,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=32,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=my_pretrained_model,\n",
       "save_on_each_node=False,\n",
       "save_steps=30,\n",
       "save_strategy=IntervalStrategy.STEPS,\n",
       "save_total_limit=2,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0fe70319",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=datacollator,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a3e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
