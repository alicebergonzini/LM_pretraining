{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd41ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import requests\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263daf17",
   "metadata": {},
   "source": [
    "### PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dedfb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>readability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>La posizione di Bandiera Rossa.</td>\n",
       "      <td>17.622770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Non mancò di pubblicare una raccolta di rime, ...</td>\n",
       "      <td>88.351943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Considerava Peter come un figlio, sotto le cui...</td>\n",
       "      <td>86.028765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Sfortunatamente, tra il 1997 e il giugno 2001,...</td>\n",
       "      <td>85.779941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>Terminata la prima guerra mondiale, che aveva ...</td>\n",
       "      <td>99.222837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id                                               text  \\\n",
       "0           1   2                    La posizione di Bandiera Rossa.   \n",
       "1           2   3  Non mancò di pubblicare una raccolta di rime, ...   \n",
       "2           3   4  Considerava Peter come un figlio, sotto le cui...   \n",
       "3           4   5  Sfortunatamente, tra il 1997 e il giugno 2001,...   \n",
       "4           6   7  Terminata la prima guerra mondiale, che aveva ...   \n",
       "\n",
       "   readability  \n",
       "0    17.622770  \n",
       "1    88.351943  \n",
       "2    86.028765  \n",
       "3    85.779941  \n",
       "4    99.222837  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#si crea un dataframe con una riga per frase, attributi: id, testo e indice di gulpease\n",
    "ds_df = pd.read_csv(\"ds_leggibilita.csv\")\n",
    "ds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f599d941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>readability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>La posizione di Bandiera Rossa.</td>\n",
       "      <td>17.622770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Non mancò di pubblicare una raccolta di rime, ...</td>\n",
       "      <td>88.351943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Considerava Peter come un figlio, sotto le cui...</td>\n",
       "      <td>86.028765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Sfortunatamente, tra il 1997 e il giugno 2001,...</td>\n",
       "      <td>85.779941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Terminata la prima guerra mondiale, che aveva ...</td>\n",
       "      <td>99.222837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7672</th>\n",
       "      <td>9995</td>\n",
       "      <td>\"La Ricciarda\" (tragedia lirica;</td>\n",
       "      <td>90.633286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7673</th>\n",
       "      <td>9996</td>\n",
       "      <td>All'interno di una coppia, nella quale entramb...</td>\n",
       "      <td>74.484438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7674</th>\n",
       "      <td>9997</td>\n",
       "      <td>Enric Morera fu il musicista più dotato per il...</td>\n",
       "      <td>30.300036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7675</th>\n",
       "      <td>9998</td>\n",
       "      <td>\"Chiu-ling Shan\") sono una catena montuosa sit...</td>\n",
       "      <td>45.706033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7676</th>\n",
       "      <td>9999</td>\n",
       "      <td>Alexander Ghent e la moglie Karenna.</td>\n",
       "      <td>4.792815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7677 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  readability\n",
       "0        2                    La posizione di Bandiera Rossa.    17.622770\n",
       "1        3  Non mancò di pubblicare una raccolta di rime, ...    88.351943\n",
       "2        4  Considerava Peter come un figlio, sotto le cui...    86.028765\n",
       "3        5  Sfortunatamente, tra il 1997 e il giugno 2001,...    85.779941\n",
       "4        7  Terminata la prima guerra mondiale, che aveva ...    99.222837\n",
       "...    ...                                                ...          ...\n",
       "7672  9995                   \"La Ricciarda\" (tragedia lirica;    90.633286\n",
       "7673  9996  All'interno di una coppia, nella quale entramb...    74.484438\n",
       "7674  9997  Enric Morera fu il musicista più dotato per il...    30.300036\n",
       "7675  9998  \"Chiu-ling Shan\") sono una catena montuosa sit...    45.706033\n",
       "7676  9999               Alexander Ghent e la moglie Karenna.     4.792815\n",
       "\n",
       "[7677 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7563c918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Divido in training e test set\n",
    "dataset = Dataset.from_pandas(ds_df)\n",
    "split_set = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_ds = split_set[\"train\"]\n",
    "test_ds = split_set[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e9bd3",
   "metadata": {},
   "source": [
    "### TOKENIZATION\n",
    "\n",
    "In questa sezione si importa il tokenizzatore col quale si tokenizza ciascuna frase nel formato necessario per Bert, alla fine si otterrà un dataset nel formato corretto con tutte le features necessarie per il training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc991e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0317bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#si importa il tokenizzatore già configurato (in questo caso: bert-base-italian-cased)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-italian-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6a74b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319ab6919e4c46c991c89472120d1891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6909 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30872239821466c890d4ceb3e8161ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#facciamo l'encoding di tutto il dataset tokenizzando frase per frase\n",
    "def encode(sample):\n",
    "    return tokenizer(sample[\"text\"], padding=True, truncation=True, max_length=512, return_special_tokens_mask=True)\n",
    "\n",
    "train_set = train_ds.map(encode, batched=True)\n",
    "test_set = test_ds.map(encode, batched=True)\n",
    "train_set.set_format('torch', columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"])\n",
    "test_set.set_format('torch', columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "643587d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'id', 'text', 'readability', 'input_ids', 'token_type_ids', 'special_tokens_mask', 'attention_mask'],\n",
       "    num_rows: 6909\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95290375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'id', 'text', 'readability', 'input_ids', 'token_type_ids', 'special_tokens_mask', 'attention_mask'],\n",
       "    num_rows: 768\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def829e",
   "metadata": {},
   "source": [
    "### TRAINING DI BERT\n",
    "\n",
    "Si procede al training di Bert. Il modello dovrà partire da uno stato iniziale con pesi random, per questo non si importa il modello già addestrato, ma si configura semplicemente l'architettura la sua architettura per poi addestrarlo da zero. Si definisce poi una strategia di training e i suoi argomenti per poi addestrare il modello sul trask di Language Modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93b20d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, TrainerCallback, BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling, set_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54c37d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"prajjwal1/bert-mini\"\n",
    "model_config = BertConfig.from_pretrained(model_name)\n",
    "\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2348df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(31102, 256)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForMaskedLM(model_config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abd13ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1024,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 4,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.20.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 31102\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09353207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usiamo il datacollator per fare le batch per il training\n",
    "datacollator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.2, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc554e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=PreTrainedTokenizer(name_or_path='dbmdz/bert-base-italian-cased', vocab_size=31102, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), mlm=True, mlm_probability=0.2, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datacollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90322bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lunghezza del dataset: 6909\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lunghezza del dataset: {len(train_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95a4059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definisco una funzione di callback per verificare l'ordinamento dei dati per ogni epoca\n",
    "from transformers.trainer_callback import TrainerControl, TrainerState\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "\n",
    "class check_ds_order(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, train_dataloader, **kwargs):\n",
    "        f = open(f\"train_check.txt\", \"a\")\n",
    "        f.write(f\"\\n------------------------ ORDINE DEI DATI ALL'INIZI DELL'EPOCA {int(state.epoch+1)} ------------------------\")\n",
    "        f.write(str(train_dataloader.dataset[\"input_ids\"][:5]))\n",
    "        f.write(\"-----------------------------------------------------------------------------------\")\n",
    "\n",
    "class check_weights(TrainerCallback):\n",
    "    def on_train_begin(self, args, state, control, model, **kwargs):\n",
    "        init_weights = model.state_dict()\n",
    "        for key, value in init_weights.items():\n",
    "            print(f\"\\n{key}:\\n\")\n",
    "            print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0a3b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#argomenti provvisori, da definire meglio\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"my_pretrained_model\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=42, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "662c1aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad9e34f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "open('train_check.txt', 'w').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a14c39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=datacollator,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    callbacks=[check_ds_order, check_weights], \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fc08b00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask, id, text, readability, Unnamed: 0. If special_tokens_mask, id, text, readability, Unnamed: 0 are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\bergo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6909\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bert.embeddings.position_ids:\n",
      "\n",
      "tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "         140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "         154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "         168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "         182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "         196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "         210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "         224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "         238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "         252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "         266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "         280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "         294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "         308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "         322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "         336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "         350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "         364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "         378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "         392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "         406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "         420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "         434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "         448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
      "         462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
      "         476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
      "         490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
      "         504, 505, 506, 507, 508, 509, 510, 511]])\n",
      "\n",
      "bert.embeddings.word_embeddings.weight:\n",
      "\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0286, -0.0439, -0.0120,  ...,  0.0197,  0.0061,  0.0300],\n",
      "        [ 0.0033, -0.0187, -0.0395,  ...,  0.0084,  0.0068,  0.0176],\n",
      "        ...,\n",
      "        [ 0.0032, -0.0142,  0.0201,  ...,  0.0018, -0.0311,  0.0348],\n",
      "        [ 0.0045,  0.0146, -0.0114,  ...,  0.0066,  0.0119,  0.0082],\n",
      "        [-0.0237,  0.0264, -0.0202,  ..., -0.0403, -0.0376,  0.0255]])\n",
      "\n",
      "bert.embeddings.position_embeddings.weight:\n",
      "\n",
      "tensor([[-2.3391e-04,  1.2967e-02,  1.8522e-02,  ...,  4.9568e-03,\n",
      "          2.5894e-02, -2.6781e-05],\n",
      "        [ 7.2665e-03,  1.5119e-02,  5.1762e-02,  ...,  3.8366e-02,\n",
      "         -2.1624e-02, -2.4307e-02],\n",
      "        [-8.5117e-03, -2.2108e-02,  2.5035e-03,  ...,  3.1694e-02,\n",
      "         -2.8386e-02, -2.2540e-02],\n",
      "        ...,\n",
      "        [ 2.0462e-02, -1.2192e-02, -4.9268e-03,  ...,  1.3451e-02,\n",
      "         -2.4105e-03,  1.1385e-02],\n",
      "        [-5.2150e-03,  1.2646e-02,  2.4589e-02,  ..., -1.9966e-02,\n",
      "          1.2553e-02, -2.2020e-02],\n",
      "        [ 2.1897e-03, -4.0505e-02, -3.4797e-02,  ...,  3.3924e-02,\n",
      "          2.1021e-02,  3.3896e-02]])\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight:\n",
      "\n",
      "tensor([[-5.6889e-03,  1.1825e-02,  6.1876e-03, -3.5470e-02,  6.9850e-02,\n",
      "          7.7111e-03, -1.9953e-02, -1.3073e-02, -1.7482e-02, -1.7541e-02,\n",
      "          2.5566e-02,  1.3025e-02,  1.1420e-03, -1.8078e-02,  9.1602e-03,\n",
      "          2.0573e-02,  1.3685e-02,  2.1319e-02, -2.1511e-02,  2.2540e-02,\n",
      "         -2.2055e-02,  5.2060e-02,  1.5341e-02,  9.5583e-03, -2.9297e-02,\n",
      "         -3.3651e-02, -5.3526e-03,  8.7381e-03, -2.2060e-02,  9.3131e-04,\n",
      "          1.5137e-02,  2.0534e-02,  4.5967e-02, -3.0157e-02, -7.7837e-03,\n",
      "          1.1391e-02, -5.6440e-03,  1.6164e-02, -2.9837e-02,  1.0222e-02,\n",
      "         -6.6410e-03, -6.0041e-03,  8.6849e-03,  4.5140e-03, -4.2861e-02,\n",
      "         -9.2703e-05, -1.8720e-02,  1.7560e-02, -2.8106e-02, -2.4360e-02,\n",
      "          1.8809e-02, -1.5520e-02, -2.1530e-02,  7.7340e-03,  9.0232e-03,\n",
      "         -1.1919e-03, -3.8452e-03,  1.2247e-02, -2.8752e-03, -7.7268e-03,\n",
      "         -3.1470e-02, -3.0860e-03,  3.9506e-02,  8.5162e-03,  2.2015e-02,\n",
      "         -2.9047e-02, -1.2167e-02, -3.4138e-03, -1.7678e-02,  1.3312e-02,\n",
      "         -6.1335e-04, -3.8120e-02, -4.1927e-03, -5.7082e-03, -9.7669e-03,\n",
      "         -3.4024e-03, -4.6025e-02, -2.8679e-02,  1.0247e-02,  4.0368e-03,\n",
      "          3.9007e-02, -1.3457e-02,  2.5148e-02,  1.0383e-02,  3.0892e-02,\n",
      "         -1.0750e-02, -1.9566e-02,  1.7353e-02, -1.8682e-02,  2.0983e-02,\n",
      "          2.5872e-03, -9.1116e-03,  1.2768e-02,  1.4787e-02,  6.9974e-03,\n",
      "          2.4096e-02, -2.9301e-02, -1.4353e-02,  2.8007e-02,  2.6652e-02,\n",
      "         -8.1520e-03,  1.3997e-02,  8.2721e-03, -2.0233e-02, -1.0514e-02,\n",
      "         -1.4719e-02, -2.1197e-03,  2.3745e-02, -8.8571e-03,  1.1499e-02,\n",
      "          7.2348e-03,  1.1468e-02,  2.6239e-02,  7.8321e-03, -1.8181e-02,\n",
      "          1.5544e-02, -1.6032e-02,  9.3835e-03, -1.1088e-02,  2.0851e-02,\n",
      "         -1.5120e-03, -8.7691e-03,  2.2784e-02, -3.3556e-02, -5.5331e-03,\n",
      "          1.0863e-02, -5.0513e-04,  2.6925e-02, -2.7031e-03, -1.8257e-02,\n",
      "          1.6376e-02,  2.3142e-04,  1.5432e-02, -9.2278e-03, -8.6550e-03,\n",
      "         -2.6859e-02, -1.5212e-03, -1.9676e-02,  8.9552e-03,  9.0436e-03,\n",
      "          1.1006e-02,  3.2550e-02, -2.4328e-02, -1.3697e-02,  1.6563e-02,\n",
      "         -1.9161e-02, -5.1122e-03,  2.7176e-02,  1.6873e-02,  1.8572e-02,\n",
      "          5.1374e-03,  2.5860e-02,  4.7309e-02,  6.7940e-03, -8.6048e-03,\n",
      "         -4.1350e-02,  2.1109e-02, -2.9626e-02,  3.7476e-02, -1.0948e-02,\n",
      "         -9.3775e-03,  3.2275e-02,  9.7354e-03, -1.2636e-03, -4.9419e-03,\n",
      "          1.3938e-03, -1.4776e-02, -3.1190e-03,  4.4926e-02, -1.7825e-02,\n",
      "          3.7210e-02,  1.4802e-02,  2.8376e-02,  1.5702e-02,  4.0094e-02,\n",
      "          8.4473e-03,  2.9297e-02, -7.2329e-03,  1.5344e-02, -2.3727e-03,\n",
      "          1.2863e-02, -1.1663e-03,  1.4103e-02, -2.1306e-02,  1.3079e-03,\n",
      "          1.7597e-02,  1.3275e-02, -9.0642e-04,  3.5274e-04,  2.4310e-02,\n",
      "         -3.8162e-02,  1.0916e-02,  1.5191e-02, -9.1122e-03, -3.0996e-02,\n",
      "         -5.3424e-03,  2.9271e-02,  2.1474e-02,  4.8235e-03, -1.6630e-02,\n",
      "          2.7613e-03, -1.0674e-02,  4.3510e-03, -4.0162e-04, -3.0035e-02,\n",
      "         -3.3489e-02,  1.3177e-02, -2.2827e-02,  2.3400e-02, -1.9534e-02,\n",
      "         -1.1541e-02, -2.5279e-02,  7.2867e-03,  1.3014e-02, -2.1897e-02,\n",
      "         -1.3631e-02,  3.0885e-04, -5.3763e-03,  4.5322e-02, -3.8623e-03,\n",
      "          1.5752e-02, -2.5053e-02,  2.1962e-02,  3.2362e-02,  5.3207e-03,\n",
      "         -1.0691e-02, -8.6690e-03,  1.7695e-02, -1.2374e-02,  9.3478e-03,\n",
      "         -2.2086e-02,  4.6227e-02, -2.1903e-02,  1.9096e-02,  2.4611e-03,\n",
      "          3.2359e-02, -5.6116e-04, -4.9848e-04,  2.7984e-02,  4.0517e-04,\n",
      "          8.2185e-05,  8.7258e-03,  1.4819e-02, -1.9878e-02,  1.2021e-04,\n",
      "          1.6776e-02,  1.4002e-02,  1.9306e-03, -2.7181e-02, -2.0900e-02,\n",
      "         -4.0286e-03, -2.2992e-02,  2.3442e-02,  1.8329e-02,  6.1307e-03,\n",
      "          3.3887e-02],\n",
      "        [ 1.6505e-02,  1.4177e-02, -1.3566e-02, -6.5462e-03, -7.1344e-04,\n",
      "         -1.6360e-02, -1.6214e-02, -1.2674e-02,  2.8158e-02,  1.7430e-02,\n",
      "          7.5557e-03,  2.1115e-02,  1.7787e-02,  8.9597e-03,  4.5196e-03,\n",
      "         -2.7956e-02,  8.5357e-03, -1.5287e-03, -1.8342e-02, -2.8248e-02,\n",
      "          3.6160e-03,  1.2399e-02, -2.1059e-03, -1.2376e-02,  4.1809e-03,\n",
      "         -6.9346e-03,  8.2127e-03, -7.0085e-03, -5.0699e-03, -9.3535e-03,\n",
      "          5.8895e-05, -5.0573e-03, -1.1512e-02,  6.5536e-03, -2.1558e-02,\n",
      "          9.5919e-03, -1.2288e-02, -2.0117e-02, -2.3709e-02,  3.0064e-02,\n",
      "          1.4803e-02,  1.2185e-02,  6.3461e-03,  2.2593e-02, -1.4637e-02,\n",
      "         -2.2337e-02, -1.3869e-02, -1.9698e-02,  1.0849e-02,  1.5016e-02,\n",
      "         -1.9762e-02,  8.5245e-03,  5.8908e-03,  1.3625e-02,  3.9158e-03,\n",
      "          3.0602e-02, -1.6810e-02,  2.1037e-02, -1.5884e-02, -1.0084e-02,\n",
      "          5.0282e-02,  1.4278e-02,  1.7303e-02, -5.9619e-03,  4.6942e-03,\n",
      "         -1.1265e-02,  4.2070e-02, -1.9802e-02, -7.3492e-03, -4.8508e-03,\n",
      "         -2.1937e-02,  1.9342e-02, -5.1643e-03,  1.7382e-03, -1.3679e-02,\n",
      "          2.1585e-02,  1.9067e-02,  1.6794e-02, -3.4653e-02,  1.3039e-02,\n",
      "          1.3062e-02, -1.4469e-02,  2.0274e-02,  1.8113e-03, -3.9614e-02,\n",
      "          2.1463e-02,  2.3852e-02,  1.2911e-02, -8.1471e-03, -8.6494e-03,\n",
      "         -2.9848e-03, -8.1568e-03, -1.3798e-03, -2.2521e-02,  3.6979e-02,\n",
      "         -1.7042e-02, -5.8035e-03, -4.4978e-04, -1.6992e-02, -1.3417e-02,\n",
      "          1.8611e-02, -1.3453e-02, -1.2808e-02, -7.4649e-03, -1.9757e-03,\n",
      "          5.0266e-03,  1.6375e-02,  3.6770e-02, -4.3010e-02, -2.0107e-02,\n",
      "          8.8320e-03,  3.1044e-02,  4.3319e-02,  2.5606e-02, -4.6022e-03,\n",
      "          5.9852e-03,  4.3144e-02, -1.0373e-02, -6.4068e-03,  6.8907e-03,\n",
      "          1.2879e-02, -2.2500e-02,  3.6547e-02,  1.6858e-02, -1.0805e-02,\n",
      "         -2.4469e-03,  2.5485e-02,  4.1261e-03, -1.2372e-02, -3.1676e-03,\n",
      "          1.1265e-02,  1.3266e-02,  1.2859e-02, -1.7624e-02,  3.5447e-02,\n",
      "         -1.3033e-02, -2.6542e-02, -5.4515e-03,  3.5330e-02,  3.4043e-02,\n",
      "          1.0372e-02,  9.7623e-03,  1.3402e-02,  2.9718e-03, -3.6712e-02,\n",
      "         -2.2564e-02,  1.0972e-02, -1.1619e-02, -3.6680e-03,  4.2313e-02,\n",
      "          2.2379e-02, -2.1441e-02,  2.3020e-02, -2.7410e-02, -1.4134e-02,\n",
      "          2.4302e-02, -4.8061e-02, -1.1246e-02, -2.0354e-02,  1.6305e-03,\n",
      "         -2.7840e-02, -3.8674e-03,  2.2189e-02,  5.6761e-03,  1.1546e-02,\n",
      "         -8.6199e-04,  8.8769e-03,  1.2750e-02, -7.5894e-03, -8.0532e-03,\n",
      "          5.0362e-03,  1.8281e-03, -4.9895e-05, -1.5334e-02, -1.4557e-02,\n",
      "          7.3270e-03, -1.5437e-02,  1.2226e-02,  3.0965e-03,  2.0903e-02,\n",
      "          1.9344e-02,  8.9866e-04, -5.5641e-02, -8.1997e-03, -5.6915e-03,\n",
      "         -2.9780e-02,  1.9140e-02, -1.8726e-02,  2.3369e-02,  7.3828e-03,\n",
      "          2.1015e-02,  6.1686e-03, -6.4145e-03, -1.4533e-02, -2.1158e-02,\n",
      "         -6.0780e-03, -1.9150e-02,  2.2900e-02,  2.1657e-02,  4.5042e-03,\n",
      "         -2.5352e-02, -4.1723e-03,  3.0352e-02, -4.3636e-02, -1.4625e-02,\n",
      "         -2.5120e-02, -2.5930e-02,  1.4994e-02, -5.9778e-03, -2.1652e-02,\n",
      "          2.2207e-02, -6.9814e-03, -2.8653e-02, -3.4043e-03,  1.7627e-02,\n",
      "          6.4170e-03,  1.7929e-02, -8.6910e-03, -1.5463e-02, -1.1697e-02,\n",
      "         -5.4950e-03, -3.2879e-02,  1.7091e-03, -1.4241e-03, -1.9397e-02,\n",
      "          4.7126e-03, -1.5409e-02, -3.2918e-02, -3.2096e-02, -1.0840e-02,\n",
      "         -1.4475e-03,  2.3126e-02, -1.6592e-03, -9.6315e-03,  5.4396e-03,\n",
      "          1.5609e-02, -4.8216e-03,  5.6210e-03, -1.7541e-02, -6.5089e-02,\n",
      "          2.4369e-02,  3.4690e-02,  3.4934e-02,  5.8945e-03,  4.1349e-03,\n",
      "          5.0599e-04,  1.5597e-02, -3.9653e-02, -1.9221e-02, -2.8568e-02,\n",
      "         -1.6465e-03,  1.5942e-02,  3.9487e-03, -2.9160e-02,  4.7436e-03,\n",
      "          1.0336e-02]])\n",
      "\n",
      "bert.embeddings.LayerNorm.weight:\n",
      "\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.])\n",
      "\n",
      "bert.embeddings.LayerNorm.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight:\n",
      "\n",
      "tensor([[ 0.0208, -0.0017, -0.0005,  ...,  0.0298,  0.0291, -0.0076],\n",
      "        [-0.0075, -0.0019, -0.0071,  ...,  0.0105, -0.0009,  0.0141],\n",
      "        [ 0.0217,  0.0220,  0.0215,  ...,  0.0036, -0.0284,  0.0149],\n",
      "        ...,\n",
      "        [ 0.0139, -0.0249, -0.0465,  ...,  0.0407,  0.0010, -0.0099],\n",
      "        [ 0.0235,  0.0161, -0.0052,  ...,  0.0192,  0.0183, -0.0044],\n",
      "        [ 0.0104,  0.0051,  0.0211,  ..., -0.0176, -0.0194, -0.0015]])\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight:\n",
      "\n",
      "tensor([[-2.1647e-02,  7.9103e-03, -4.3519e-02,  ..., -4.7770e-02,\n",
      "         -3.2192e-03,  6.1889e-03],\n",
      "        [-2.3188e-02,  2.0569e-02, -7.9424e-03,  ..., -9.6406e-03,\n",
      "          2.6467e-02, -1.6139e-02],\n",
      "        [ 2.6522e-02, -1.6202e-03, -1.1522e-02,  ...,  1.8884e-02,\n",
      "         -7.3513e-05, -4.8491e-03],\n",
      "        ...,\n",
      "        [ 1.4946e-02,  1.0874e-02, -2.4520e-02,  ..., -1.0372e-02,\n",
      "          1.5782e-02,  3.6319e-02],\n",
      "        [ 1.6659e-03,  6.0639e-03, -2.3293e-02,  ...,  1.6569e-02,\n",
      "          3.1570e-02, -4.7761e-03],\n",
      "        [ 9.5543e-03,  8.1348e-03,  1.2606e-02,  ...,  7.3630e-03,\n",
      "         -8.3676e-03, -4.6983e-03]])\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight:\n",
      "\n",
      "tensor([[-9.3078e-03,  5.8712e-03, -1.7860e-02,  ...,  2.2289e-02,\n",
      "         -3.0498e-02, -2.4559e-02],\n",
      "        [-1.0980e-02, -3.9026e-02, -4.3764e-05,  ...,  4.8726e-03,\n",
      "         -1.4731e-02,  1.6505e-02],\n",
      "        [-2.9458e-02,  6.5889e-04,  2.7687e-03,  ..., -1.8094e-02,\n",
      "          1.3591e-02,  2.0020e-02],\n",
      "        ...,\n",
      "        [-3.3462e-02, -2.2063e-02,  1.1955e-02,  ...,  3.1645e-03,\n",
      "         -1.8167e-02, -1.1420e-02],\n",
      "        [-2.1617e-02, -2.1794e-02,  3.1827e-02,  ..., -1.5708e-02,\n",
      "         -1.6433e-03, -3.7445e-02],\n",
      "        [ 3.8315e-02,  2.5097e-02, -6.7186e-03,  ...,  1.1317e-02,\n",
      "         -6.9951e-03,  1.4489e-02]])\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight:\n",
      "\n",
      "tensor([[ 0.0122, -0.0009,  0.0197,  ...,  0.0347,  0.0095, -0.0202],\n",
      "        [-0.0181, -0.0126, -0.0129,  ...,  0.0015,  0.0158, -0.0043],\n",
      "        [ 0.0197, -0.0303,  0.0396,  ..., -0.0053,  0.0089,  0.0060],\n",
      "        ...,\n",
      "        [-0.0048,  0.0030,  0.0042,  ..., -0.0102, -0.0141,  0.0035],\n",
      "        [-0.0437,  0.0246,  0.0269,  ..., -0.0238, -0.0173,  0.0085],\n",
      "        [ 0.0022, -0.0403,  0.0093,  ..., -0.0075,  0.0205,  0.0099]])\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight:\n",
      "\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.])\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight:\n",
      "\n",
      "tensor([[-0.0056,  0.0218, -0.0287,  ..., -0.0130, -0.0247, -0.0548],\n",
      "        [-0.0030,  0.0157,  0.0046,  ..., -0.0174,  0.0177, -0.0038],\n",
      "        [-0.0163,  0.0181, -0.0296,  ...,  0.0081,  0.0112,  0.0285],\n",
      "        ...,\n",
      "        [-0.0355, -0.0171,  0.0325,  ...,  0.0002,  0.0048,  0.0149],\n",
      "        [ 0.0142,  0.0177,  0.0303,  ..., -0.0014,  0.0189, -0.0070],\n",
      "        [-0.0206, -0.0170, -0.0243,  ..., -0.0063,  0.0182,  0.0047]])\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias:\n",
      "\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight:\n",
      "\n",
      "tensor([[-0.0275, -0.0088, -0.0229,  ...,  0.0221, -0.0228, -0.0299],\n",
      "        [ 0.0494,  0.0319,  0.0266,  ...,  0.0231, -0.0149, -0.0285],\n",
      "        [ 0.0215, -0.0450, -0.0373,  ..., -0.0216, -0.0462,  0.0083],\n",
      "        ...,\n",
      "        [-0.0206,  0.0222, -0.0108,  ..., -0.0369, -0.0212, -0.0119],\n",
      "        [ 0.0385, -0.0040,  0.0107,  ..., -0.0335, -0.0200,  0.0002],\n",
      "        [-0.0039, -0.0030, -0.0212,  ..., -0.0193, -0.0201,  0.0317]])\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight:\n",
      "\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.])\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.1.attention.self.query.weight:\n",
      "\n",
      "tensor([[-0.0276,  0.0399, -0.0051,  ...,  0.0106, -0.0348,  0.0109],\n",
      "        [ 0.0105, -0.0017,  0.0022,  ...,  0.0076, -0.0445, -0.0100],\n",
      "        [ 0.0320, -0.0097,  0.0157,  ...,  0.0037,  0.0057, -0.0008],\n",
      "        ...,\n",
      "        [-0.0221,  0.0096, -0.0191,  ...,  0.0107, -0.0070, -0.0144],\n",
      "        [ 0.0219, -0.0456, -0.0197,  ...,  0.0140, -0.0036, -0.0060],\n",
      "        [ 0.0443,  0.0230, -0.0199,  ...,  0.0281,  0.0397,  0.0243]])\n",
      "\n",
      "bert.encoder.layer.1.attention.self.query.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.1.attention.self.key.weight:\n",
      "\n",
      "tensor([[-0.0340,  0.0137, -0.0337,  ..., -0.0034, -0.0139, -0.0185],\n",
      "        [ 0.0007, -0.0071,  0.0182,  ..., -0.0165,  0.0044,  0.0243],\n",
      "        [ 0.0226,  0.0156,  0.0085,  ..., -0.0425, -0.0108,  0.0325],\n",
      "        ...,\n",
      "        [ 0.0095,  0.0083,  0.0328,  ...,  0.0024,  0.0383,  0.0131],\n",
      "        [ 0.0087, -0.0223, -0.0055,  ...,  0.0233, -0.0064,  0.0061],\n",
      "        [ 0.0160,  0.0077, -0.0121,  ...,  0.0147, -0.0046, -0.0038]])\n",
      "\n",
      "bert.encoder.layer.1.attention.self.key.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.1.attention.self.value.weight:\n",
      "\n",
      "tensor([[-0.0180, -0.0062,  0.0099,  ...,  0.0099, -0.0018, -0.0076],\n",
      "        [-0.0055,  0.0147, -0.0476,  ..., -0.0080, -0.0242, -0.0124],\n",
      "        [ 0.0109, -0.0080,  0.0419,  ..., -0.0095, -0.0382,  0.0207],\n",
      "        ...,\n",
      "        [-0.0125,  0.0209, -0.0069,  ..., -0.0064,  0.0261, -0.0374],\n",
      "        [ 0.0063,  0.0140,  0.0117,  ...,  0.0010,  0.0295,  0.0162],\n",
      "        [-0.0119, -0.0381, -0.0093,  ..., -0.0255, -0.0161,  0.0343]])\n",
      "\n",
      "bert.encoder.layer.1.attention.self.value.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.1.attention.output.dense.weight:\n",
      "\n",
      "tensor([[ 0.0120, -0.0101,  0.0290,  ..., -0.0078,  0.0105,  0.0010],\n",
      "        [ 0.0111,  0.0311, -0.0146,  ...,  0.0078, -0.0385,  0.0216],\n",
      "        [ 0.0217,  0.0013, -0.0050,  ..., -0.0149,  0.0004,  0.0080],\n",
      "        ...,\n",
      "        [-0.0015,  0.0093, -0.0009,  ...,  0.0037,  0.0201,  0.0191],\n",
      "        [ 0.0322, -0.0033, -0.0068,  ...,  0.0244, -0.0079, -0.0068],\n",
      "        [ 0.0270,  0.0010,  0.0088,  ..., -0.0133, -0.0112,  0.0118]])\n",
      "\n",
      "bert.encoder.layer.1.attention.output.dense.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight:\n",
      "\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.])\n",
      "\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.1.intermediate.dense.weight:\n",
      "\n",
      "tensor([[-0.0530,  0.0142,  0.0173,  ...,  0.0131,  0.0086, -0.0076],\n",
      "        [-0.0056, -0.0108, -0.0289,  ..., -0.0072, -0.0013, -0.0156],\n",
      "        [ 0.0114, -0.0148,  0.0542,  ...,  0.0195, -0.0217,  0.0309],\n",
      "        ...,\n",
      "        [ 0.0022, -0.0301, -0.0187,  ...,  0.0134, -0.0202,  0.0194],\n",
      "        [-0.0225,  0.0043,  0.0019,  ...,  0.0082, -0.0011, -0.0011],\n",
      "        [ 0.0226, -0.0108,  0.0195,  ...,  0.0305, -0.0002,  0.0123]])\n",
      "\n",
      "bert.encoder.layer.1.intermediate.dense.bias:\n",
      "\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.1.output.dense.weight:\n",
      "\n",
      "tensor([[ 0.0362,  0.0281, -0.0082,  ..., -0.0212,  0.0276,  0.0260],\n",
      "        [ 0.0335, -0.0316, -0.0385,  ..., -0.0024, -0.0421, -0.0032],\n",
      "        [-0.0174, -0.0009, -0.0169,  ...,  0.0158,  0.0270, -0.0287],\n",
      "        ...,\n",
      "        [-0.0044, -0.0030, -0.0194,  ...,  0.0232,  0.0108, -0.0017],\n",
      "        [-0.0315, -0.0239, -0.0179,  ...,  0.0194, -0.0046, -0.0047],\n",
      "        [ 0.0354, -0.0081,  0.0160,  ...,  0.0007, -0.0065,  0.0155]])\n",
      "\n",
      "bert.encoder.layer.1.output.dense.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.1.output.LayerNorm.weight:\n",
      "\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.])\n",
      "\n",
      "bert.encoder.layer.1.output.LayerNorm.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.2.attention.self.query.weight:\n",
      "\n",
      "tensor([[-0.0300, -0.0102, -0.0107,  ...,  0.0082, -0.0057,  0.0163],\n",
      "        [ 0.0099, -0.0065, -0.0134,  ..., -0.0156,  0.0114, -0.0163],\n",
      "        [-0.0358, -0.0124, -0.0047,  ...,  0.0014,  0.0019,  0.0052],\n",
      "        ...,\n",
      "        [ 0.0042,  0.0158,  0.0183,  ...,  0.0083, -0.0219, -0.0194],\n",
      "        [-0.0246, -0.0062,  0.0179,  ...,  0.0254, -0.0051, -0.0235],\n",
      "        [ 0.0073, -0.0037, -0.0273,  ...,  0.0140,  0.0052, -0.0102]])\n",
      "\n",
      "bert.encoder.layer.2.attention.self.query.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.2.attention.self.key.weight:\n",
      "\n",
      "tensor([[-0.0116, -0.0231,  0.0074,  ...,  0.0186, -0.0245,  0.0137],\n",
      "        [ 0.0032,  0.0253,  0.0332,  ..., -0.0347, -0.0217, -0.0138],\n",
      "        [-0.0369,  0.0080, -0.0063,  ...,  0.0423, -0.0115,  0.0072],\n",
      "        ...,\n",
      "        [ 0.0132, -0.0189,  0.0181,  ..., -0.0688, -0.0053, -0.0059],\n",
      "        [-0.0288, -0.0149, -0.0101,  ...,  0.0373, -0.0232, -0.0090],\n",
      "        [-0.0053, -0.0058,  0.0280,  ...,  0.0158,  0.0025, -0.0405]])\n",
      "\n",
      "bert.encoder.layer.2.attention.self.key.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.2.attention.self.value.weight:\n",
      "\n",
      "tensor([[-0.0053, -0.0176,  0.0033,  ..., -0.0330, -0.0035, -0.0172],\n",
      "        [ 0.0164, -0.0228, -0.0250,  ..., -0.0289, -0.0378,  0.0194],\n",
      "        [-0.0146, -0.0075, -0.0074,  ...,  0.0205,  0.0250,  0.0303],\n",
      "        ...,\n",
      "        [-0.0024, -0.0282,  0.0299,  ...,  0.0023, -0.0051, -0.0211],\n",
      "        [ 0.0120,  0.0264,  0.0103,  ..., -0.0113,  0.0117,  0.0086],\n",
      "        [-0.0504, -0.0025,  0.0264,  ...,  0.0174,  0.0181,  0.0305]])\n",
      "\n",
      "bert.encoder.layer.2.attention.self.value.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.2.attention.output.dense.weight:\n",
      "\n",
      "tensor([[ 0.0001, -0.0054,  0.0222,  ..., -0.0159,  0.0093, -0.0171],\n",
      "        [-0.0380, -0.0086, -0.0145,  ...,  0.0382,  0.0071, -0.0430],\n",
      "        [-0.0097,  0.0123, -0.0281,  ...,  0.0050, -0.0181, -0.0009],\n",
      "        ...,\n",
      "        [-0.0234,  0.0145, -0.0006,  ...,  0.0119, -0.0183,  0.0123],\n",
      "        [ 0.0205,  0.0057, -0.0004,  ..., -0.0145,  0.0056, -0.0247],\n",
      "        [-0.0028,  0.0207,  0.0054,  ...,  0.0054,  0.0608,  0.0185]])\n",
      "\n",
      "bert.encoder.layer.2.attention.output.dense.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight:\n",
      "\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.])\n",
      "\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.2.intermediate.dense.weight:\n",
      "\n",
      "tensor([[ 0.0167, -0.0028,  0.0038,  ..., -0.0053,  0.0029,  0.0319],\n",
      "        [-0.0244,  0.0250, -0.0155,  ...,  0.0158,  0.0181,  0.0127],\n",
      "        [ 0.0064, -0.0049,  0.0049,  ..., -0.0346, -0.0146, -0.0050],\n",
      "        ...,\n",
      "        [-0.0148,  0.0132,  0.0150,  ...,  0.0054,  0.0066, -0.0226],\n",
      "        [ 0.0153,  0.0076,  0.0217,  ..., -0.0093,  0.0236,  0.0066],\n",
      "        [-0.0143,  0.0049,  0.0026,  ..., -0.0010, -0.0056, -0.0097]])\n",
      "\n",
      "bert.encoder.layer.2.intermediate.dense.bias:\n",
      "\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.2.output.dense.weight:\n",
      "\n",
      "tensor([[-0.0187, -0.0121,  0.0224,  ..., -0.0213, -0.0033,  0.0150],\n",
      "        [-0.0025,  0.0165,  0.0020,  ..., -0.0067,  0.0263,  0.0094],\n",
      "        [-0.0109,  0.0022,  0.0260,  ..., -0.0075,  0.0175,  0.0120],\n",
      "        ...,\n",
      "        [-0.0026,  0.0055,  0.0114,  ...,  0.0037,  0.0164, -0.0091],\n",
      "        [ 0.0070, -0.0049,  0.0075,  ..., -0.0186,  0.0113, -0.0064],\n",
      "        [ 0.0104,  0.0174,  0.0078,  ..., -0.0110,  0.0173,  0.0323]])\n",
      "\n",
      "bert.encoder.layer.2.output.dense.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.2.output.LayerNorm.weight:\n",
      "\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.])\n",
      "\n",
      "bert.encoder.layer.2.output.LayerNorm.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.3.attention.self.query.weight:\n",
      "\n",
      "tensor([[-0.0260, -0.0325,  0.0433,  ...,  0.0096,  0.0293,  0.0205],\n",
      "        [ 0.0204, -0.0185,  0.0357,  ...,  0.0101, -0.0090,  0.0063],\n",
      "        [-0.0177, -0.0121, -0.0131,  ...,  0.0699,  0.0396,  0.0104],\n",
      "        ...,\n",
      "        [ 0.0008, -0.0294,  0.0105,  ..., -0.0144,  0.0045, -0.0182],\n",
      "        [-0.0166,  0.0163,  0.0067,  ...,  0.0200, -0.0286,  0.0250],\n",
      "        [ 0.0129, -0.0001, -0.0348,  ...,  0.0008, -0.0081,  0.0026]])\n",
      "\n",
      "bert.encoder.layer.3.attention.self.query.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.3.attention.self.key.weight:\n",
      "\n",
      "tensor([[-1.9270e-02,  4.4945e-02, -7.1936e-02,  ...,  8.4657e-03,\n",
      "         -7.9614e-06, -1.0598e-02],\n",
      "        [-1.0186e-02,  2.7418e-02, -7.8937e-04,  ..., -2.2244e-02,\n",
      "         -1.1283e-02, -5.6008e-03],\n",
      "        [ 2.6921e-02, -1.1931e-02,  1.1684e-02,  ...,  9.9973e-03,\n",
      "         -1.8034e-02, -4.9073e-02],\n",
      "        ...,\n",
      "        [-2.3035e-02,  2.7732e-03,  1.3895e-02,  ...,  1.1851e-02,\n",
      "         -1.8730e-03,  2.1756e-03],\n",
      "        [ 5.5786e-03,  6.8155e-03, -2.0642e-02,  ...,  4.0258e-03,\n",
      "         -8.2151e-03,  1.8670e-02],\n",
      "        [-6.4147e-03, -8.3337e-03, -1.5179e-02,  ...,  3.2597e-03,\n",
      "          4.2616e-02, -2.5075e-03]])\n",
      "\n",
      "bert.encoder.layer.3.attention.self.key.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.3.attention.self.value.weight:\n",
      "\n",
      "tensor([[-0.0229,  0.0093, -0.0162,  ..., -0.0322, -0.0053, -0.0315],\n",
      "        [-0.0042, -0.0023,  0.0137,  ...,  0.0007, -0.0143,  0.0208],\n",
      "        [-0.0177, -0.0322, -0.0100,  ..., -0.0061,  0.0152,  0.0274],\n",
      "        ...,\n",
      "        [ 0.0416, -0.0059, -0.0349,  ...,  0.0087, -0.0132,  0.0133],\n",
      "        [ 0.0237,  0.0050,  0.0383,  ..., -0.0233, -0.0312,  0.0142],\n",
      "        [ 0.0251,  0.0203,  0.0029,  ..., -0.0279,  0.0125, -0.0190]])\n",
      "\n",
      "bert.encoder.layer.3.attention.self.value.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.3.attention.output.dense.weight:\n",
      "\n",
      "tensor([[-0.0036,  0.0242,  0.0047,  ...,  0.0011,  0.0136,  0.0103],\n",
      "        [ 0.0446, -0.0107, -0.0091,  ..., -0.0011, -0.0296,  0.0316],\n",
      "        [-0.0013,  0.0141,  0.0092,  ...,  0.0222,  0.0010, -0.0058],\n",
      "        ...,\n",
      "        [ 0.0062, -0.0026,  0.0102,  ...,  0.0121, -0.0040,  0.0133],\n",
      "        [ 0.0045, -0.0015, -0.0335,  ...,  0.0088,  0.0060,  0.0035],\n",
      "        [-0.0116, -0.0244, -0.0014,  ...,  0.0208,  0.0063,  0.0041]])\n",
      "\n",
      "bert.encoder.layer.3.attention.output.dense.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight:\n",
      "\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.])\n",
      "\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.3.intermediate.dense.weight:\n",
      "\n",
      "tensor([[-0.0130,  0.0092,  0.0059,  ..., -0.0660,  0.0012,  0.0160],\n",
      "        [ 0.0279,  0.0133, -0.0327,  ...,  0.0023,  0.0301, -0.0045],\n",
      "        [-0.0002,  0.0163, -0.0124,  ...,  0.0368,  0.0289, -0.0005],\n",
      "        ...,\n",
      "        [-0.0098,  0.0165, -0.0266,  ..., -0.0152,  0.0110, -0.0045],\n",
      "        [ 0.0250, -0.0305, -0.0099,  ..., -0.0166, -0.0078, -0.0108],\n",
      "        [ 0.0096, -0.0181,  0.0013,  ..., -0.0070, -0.0010,  0.0096]])\n",
      "\n",
      "bert.encoder.layer.3.intermediate.dense.bias:\n",
      "\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.3.output.dense.weight:\n",
      "\n",
      "tensor([[-0.0002, -0.0176, -0.0274,  ..., -0.0041,  0.0320,  0.0084],\n",
      "        [-0.0218,  0.0041,  0.0232,  ...,  0.0306,  0.0322, -0.0446],\n",
      "        [-0.0069,  0.0043,  0.0050,  ..., -0.0276,  0.0222, -0.0007],\n",
      "        ...,\n",
      "        [ 0.0224,  0.0092,  0.0228,  ..., -0.0368, -0.0115, -0.0081],\n",
      "        [ 0.0022,  0.0173, -0.0173,  ...,  0.0242, -0.0014, -0.0173],\n",
      "        [ 0.0415,  0.0152, -0.0256,  ..., -0.0234,  0.0121,  0.0255]])\n",
      "\n",
      "bert.encoder.layer.3.output.dense.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "bert.encoder.layer.3.output.LayerNorm.weight:\n",
      "\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.])\n",
      "\n",
      "bert.encoder.layer.3.output.LayerNorm.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "cls.predictions.bias:\n",
      "\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "\n",
      "cls.predictions.transform.dense.weight:\n",
      "\n",
      "tensor([[ 0.0017,  0.0306,  0.0127,  ..., -0.0300, -0.0066, -0.0134],\n",
      "        [-0.0420, -0.0025, -0.0040,  ..., -0.0294,  0.0110, -0.0016],\n",
      "        [ 0.0075,  0.0021,  0.0238,  ...,  0.0053,  0.0069, -0.0107],\n",
      "        ...,\n",
      "        [ 0.0067, -0.0023,  0.0100,  ...,  0.0094, -0.0022,  0.0016],\n",
      "        [-0.0055, -0.0149,  0.0093,  ...,  0.0030, -0.0087,  0.0108],\n",
      "        [ 0.0100,  0.0056, -0.0311,  ...,  0.0285,  0.0393, -0.0076]])\n",
      "\n",
      "cls.predictions.transform.dense.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "cls.predictions.transform.LayerNorm.weight:\n",
      "\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.])\n",
      "\n",
      "cls.predictions.transform.LayerNorm.bias:\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "cls.predictions.decoder.weight:\n",
      "\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0286, -0.0439, -0.0120,  ...,  0.0197,  0.0061,  0.0300],\n",
      "        [ 0.0033, -0.0187, -0.0395,  ...,  0.0084,  0.0068,  0.0176],\n",
      "        ...,\n",
      "        [ 0.0032, -0.0142,  0.0201,  ...,  0.0018, -0.0311,  0.0348],\n",
      "        [ 0.0045,  0.0146, -0.0114,  ...,  0.0066,  0.0119,  0.0082],\n",
      "        [-0.0237,  0.0264, -0.0202,  ..., -0.0403, -0.0376,  0.0255]])\n",
      "\n",
      "cls.predictions.decoder.bias:\n",
      "\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd81e761e9140c3af34756fdefd317d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e8ee48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
