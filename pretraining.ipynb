{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd41ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import requests\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263daf17",
   "metadata": {},
   "source": [
    "### PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dedfb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/data_set/train_set.csv\")\n",
    "val_df = pd.read_csv(\"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/data_set/val_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbcadcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set di dimensioni: 988721 - Variabili: ['id', 'text', 'readability', 'gulpease']\n",
      "Validation set di dimensioni: 49762 - Variabili: ['id', 'text', 'readability', 'gulpease']\n"
     ]
    }
   ],
   "source": [
    "print(f'Training set di dimensioni: {len(train_df)} - Variabili: {train_df.columns.to_list()}')\n",
    "print(f'Validation set di dimensioni: {len(val_df)} - Variabili: {val_df.columns.to_list()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7563c918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Divido in training e test set\n",
    "tr_set = Dataset.from_pandas(train_df[:10000])\n",
    "val_set = Dataset.from_pandas(val_df[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1210649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'readability', 'gulpease'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e9bd3",
   "metadata": {},
   "source": [
    "### TOKENIZATION\n",
    "\n",
    "In questa sezione si importa il tokenizzatore col quale si tokenizza ciascuna frase nel formato necessario per Bert, alla fine si otterrà un dataset nel formato corretto con tutte le features necessarie per il training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc991e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0317bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#si importa il tokenizzatore già configurato (in questo caso: bert-base-italian-cased)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-italian-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6a74b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09328a2c98b42e39acbb0d4447cd03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e291002d82f40dd89adf520bfb3fbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#facciamo l'encoding di tutto il dataset tokenizzando frase per frase\n",
    "def encode(sample):\n",
    "    return tokenizer(sample[\"text\"], padding=True, truncation=True, max_length=128, return_special_tokens_mask=True) \n",
    "\n",
    "train_set = tr_set.map(encode, batched=True)\n",
    "test_set = val_set.map(encode, batched=True)\n",
    "train_set.set_format('torch', columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"])\n",
    "test_set.set_format('torch', columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "643587d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'readability', 'gulpease', 'input_ids', 'token_type_ids', 'special_tokens_mask', 'attention_mask'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95290375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'readability', 'gulpease', 'input_ids', 'token_type_ids', 'special_tokens_mask', 'attention_mask'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def829e",
   "metadata": {},
   "source": [
    "### TRAINING DI BERT\n",
    "\n",
    "Si procede al training di Bert. Il modello dovrà partire da uno stato iniziale con pesi random, per questo non si importa il modello già addestrato, ma si configura semplicemente l'architettura la sua architettura per poi addestrarlo da zero. Si definisce poi una strategia di training e i suoi argomenti per poi addestrare il modello sul trask di Language Modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93b20d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, TrainerCallback, BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling, set_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54c37d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bergo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"prajjwal1/bert-medium\"\n",
    "model_config = BertConfig.from_pretrained(model_name)\n",
    "\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2348df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(31102, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForMaskedLM(model_config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abd13ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 512,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 2048,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 8,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 31102\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09353207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usiamo il datacollator per fare le batch per il training\n",
    "datacollator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.2, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc554e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=BertTokenizer(name_or_path='dbmdz/bert-base-italian-cased', vocab_size=31102, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t104: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}, mlm=True, mlm_probability=0.2, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datacollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95a4059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definisco alcune funzione di callback\n",
    "from transformers import TrainerCallback\n",
    "from transformers.trainer_callback import TrainerControl, TrainerState\n",
    "from transformers.training_args import TrainingArguments\n",
    "import json\n",
    "\n",
    "\n",
    "#callback per incrementare il numero di step tra ogni salvataggio\n",
    "class IncrementSaveSteps(TrainerCallback):\n",
    "    def __init__(self, increase_steps):\n",
    "        self.increase_steps = increase_steps\n",
    "        \n",
    "    def on_step_end(self, args, state, control, logs=None, **kwargs):\n",
    "        current_step = state.global_step\n",
    "        if current_step in self.increase_steps:\n",
    "            state.save_steps = current_step\n",
    "            state.logging_steps = state.save_steps\n",
    "            print(f'Changed checkpoint and logging save steps to {state.save_steps}')\n",
    "\n",
    "#callback per il salvataggio customizzato dei checkpoint\n",
    "class SaveCheckpoint(TrainerCallback):\n",
    "    def __init__(self, checkpoint_path):\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "    def on_save(self, args, state, control, logs=None, **kwargs):\n",
    "        current_step = state.global_step\n",
    "        path_name = f'{self.checkpoint_path}/checkpoint-step{current_step}'\n",
    "        if not os.path.exists(path_name):\n",
    "            os.makedirs(path_name)\n",
    "        kwargs['model'].save_pretrained(path_name)\n",
    "        state.save_to_json(os.path.join(path_name, \"trainer_state.json\"))\n",
    "        kwargs['model'].config.save_pretrained(path_name)\n",
    "        if hasattr(kwargs['model'], 'generation_config'):\n",
    "            kwargs['model'].generation_config.save_pretrained(path_name)\n",
    "        control.should_save = True\n",
    "        print(f'Checkpoint has been saved for step number {current_step}. Current step_size = {state.save_steps}')\n",
    "\n",
    "#callback per salvare le metriche a ogni evaluation\n",
    "class HistoryLogger(TrainerCallback):\n",
    "    def __init__(self, dir_path):\n",
    "        self.dir_path = os.path.join(dir_path, \"history_log.json\")\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        with open(self.dir_path, 'a') as f:\n",
    "                f.write(json.dumps(metrics) + '\\n')\n",
    "                print(f\"History log file has been updated with step's {state.global_step} metrics.\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24116870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6728f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/training/prova\"\n",
    "output_dir = os.path.join(results_path, \"my_pretrained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0a3b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#argomenti provvisori, da definire meglio\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    evaluation_strategy=None,\n",
    "    eval_strategy=\"steps\", \n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=2,\n",
    "    save_strategy=\"no\",\n",
    "    eval_steps=4, #2000\n",
    "    load_best_model_at_end=False,\n",
    "    save_steps=2,\n",
    "    seed=42, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "662c1aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a14c39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(results_path, \"checkpoints\")\n",
    "steps_increments = [8, 16, 256, 2048]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=datacollator,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[IncrementSaveSteps(steps_increments), SaveCheckpoint(checkpoint_path=checkpoint_path), HistoryLogger(dir_path=results_path)], \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fc08b00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee452848760d4f718ae84165fb596b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.4263, 'grad_norm': 3.8298041820526123, 'learning_rate': 4.968051118210863e-05, 'epoch': 0.01}\n",
      "{'loss': 10.2199, 'grad_norm': 3.412374973297119, 'learning_rate': 4.936102236421725e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e733bd9c0749af9ad480d5e0aa5966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64118243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
