{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fd41ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import requests\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263daf17",
   "metadata": {},
   "source": [
    "### DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9dedfb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#si importano i dataset\n",
    "train_df = pd.read_csv(\"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/data_set/train_set.csv\")\n",
    "val_df = pd.read_csv(\"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/data_set/val_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcadcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training set di dimensioni: {len(train_df)} - Variabili: {train_df.columns.to_list()}')\n",
    "print(f'Validation set di dimensioni: {len(val_df)} - Variabili: {val_df.columns.to_list()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c4e010fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordinamento solo in caso di addestramento su curriculum o anti-curriculum\n",
    "sorted_df = train_df.sort_values(by='gulpease', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7563c918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Trasformiamo in formato Dataset i dati precedentemente importati\n",
    "tr_set = Dataset.from_pandas(sorted_df[:5000])\n",
    "val_set = Dataset.from_pandas(val_df[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1210649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_set = tr_set.shuffle(seed=5) #shuffle se ordinamento random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_set[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e9bd3",
   "metadata": {},
   "source": [
    "### TOKENIZATION\n",
    "\n",
    "In questa sezione si importa il tokenizzatore col quale si tokenizza ciascuna frase nel formato necessario per Bert, alla fine si otterrà un dataset nel formato corretto con tutte le features necessarie per il training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cc991e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0317bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#si importa il tokenizzatore già configurato (in questo caso: bert-base-italian-cased)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-italian-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a74b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#facciamo l'encoding di tutto il dataset tokenizzando frase per frase\n",
    "def encode(sample):\n",
    "    return tokenizer(sample[\"text\"], padding=True, truncation=True, max_length=128, return_special_tokens_mask=True) \n",
    "\n",
    "\n",
    "train_set = tr_set.map(encode, batched=True)\n",
    "test_set = val_set.map(encode, batched=True)\n",
    "train_set.set_format('torch', columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"])\n",
    "test_set.set_format('torch', columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643587d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95290375",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def829e",
   "metadata": {},
   "source": [
    "### TRAINING DI BERT\n",
    "\n",
    "Si procede al training di Bert. Il modello dovrà partire da uno stato iniziale con pesi random, per questo non si importa il modello già addestrato, ma si configura semplicemente l'architettura la sua architettura per poi addestrarlo da zero. Si definisce poi una strategia di training e i suoi argomenti per poi addestrare il modello sul trask di Language Modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "93b20d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, TrainerCallback, BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling, set_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c37d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#si importano configurazione e modello\n",
    "model_name = \"prajjwal1/bert-medium\"\n",
    "model_config = BertConfig.from_pretrained(model_name)\n",
    "model = BertForMaskedLM(model_config)\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2348df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#si importa un modello già inizializzato\n",
    "model_path = \"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale\"\n",
    "model.load_state_dict(torch.load(f'{model_path}/initial_model.bin'))\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd13ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "09353207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usiamo il datacollator per fare le batch per il training\n",
    "datacollator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.2, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc554e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "95a4059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definisco alcune funzione di callback\n",
    "from transformers import TrainerCallback\n",
    "from transformers.trainer_callback import TrainerControl, TrainerState\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#callback per incrementare il numero di step tra ogni salvataggio e salvataggio customizzato\n",
    "class IncrementSaveSteps(TrainerCallback):\n",
    "    def __init__(self, increase_steps, save_steps, checkpoint_path):\n",
    "        self.increase_steps = increase_steps\n",
    "        self.save_steps = save_steps\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "  \n",
    "    def on_step_end(self, args, state, control, logs=None, **kwargs):\n",
    "        current_step = state.global_step\n",
    "        if current_step % self.save_steps == 0:\n",
    "            self.save_checkpoint(state, **kwargs)\n",
    "        if current_step in self.increase_steps:\n",
    "            self.save_steps = current_step\n",
    "            state.logging_steps = self.save_steps\n",
    "            print(f'Changed checkpoint and logging steps to {self.save_steps}')\n",
    "\n",
    "    def save_checkpoint(self, state, **kwargs):\n",
    "        current_step = state.global_step\n",
    "        path_name = f'{self.checkpoint_path}/checkpoint-step{current_step}'\n",
    "        if not os.path.exists(path_name):\n",
    "            os.makedirs(path_name)\n",
    "        kwargs['model'].save_pretrained(path_name)\n",
    "        state.save_to_json(os.path.join(path_name, \"trainer_state.json\"))\n",
    "        kwargs['model'].config.save_pretrained(path_name)\n",
    "        if hasattr(kwargs['model'], 'generation_config'):\n",
    "            kwargs['model'].generation_config.save_pretrained(path_name)\n",
    "        print(f'Checkpoint has been saved for step number {current_step}. Current step_size = {self.save_steps}')\n",
    "\n",
    "\n",
    "#callback per salvare le metriche a ogni evaluation\n",
    "class HistoryLogger(TrainerCallback):\n",
    "    def __init__(self, dir_path):\n",
    "        self.dir_path = os.path.join(dir_path, \"history_log.json\")\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        with open(self.dir_path, 'a') as f:\n",
    "                f.write(json.dumps(metrics) + '\\n')\n",
    "                print(f\"History log file has been updated with step's {state.global_step} metrics.\")\n",
    "\n",
    "#callback per tensorboard\n",
    "tensorboard_callback = TensorBoardCallback()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "24116870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#da modificare per masked language modeling\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    mask = labels != -100\n",
    "    labels = labels[mask]\n",
    "    predictions = predictions[mask]\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    cross_entropy = torch.nn.CrossEntropyLoss()(torch.tensor(logits).permute(0, 2, 1), torch.tensor(labels))\n",
    "    perplexity = torch.exp(cross_entropy)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'perplexity': perplexity.item()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6728f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"C:/Users/bergo/OneDrive - University of Pisa/Tesi Magistrale/training/prova\"\n",
    "output_dir = os.path.join(results_path, \"my_pretrained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c0a3b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#argomenti per il training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    eval_strategy=\"steps\", \n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=16, #64, \n",
    "    per_device_eval_batch_size=16, #64, \n",
    "    num_train_epochs=1,\n",
    "    logging_steps=2,\n",
    "    save_strategy=\"no\",\n",
    "    eval_steps=2000,\n",
    "    load_best_model_at_end=False,\n",
    "    save_steps=2,\n",
    "    seed=42, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e26adf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(results_path, \"checkpoints\")\n",
    "steps_increments = [4, 16, 256, 2048]\n",
    "\n",
    "#si definiscono i parametri del Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=datacollator,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[IncrementSaveSteps(steps_increments, 2, checkpoint_path), HistoryLogger(dir_path=results_path), tensorboard_callback], \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc08b00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train() #si addestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64118243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
